{
  "hash": "92e21b1b541e3f8c183740f4093e2717",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlang: es\n---\n\n\n# Series de Tiempo\n\n::: {style=\"text-align: justify\"}\nLas series temporales tienen su origen en la necesidad de comprender y predecir patrones presentes en datos secuenciales a lo largo del tiempo. A lo largo de la historia, diversas civilizaciones han registrado observaciones cronológicas con el propósito de anticipar fenómenos naturales, eventos económicos y otros procesos cambiantes.\n\nNo obstante, el enfoque más sistemático en el análisis de series temporales se inició en el ámbito de la estadística y la econometría durante el siglo XX. Figuras pioneras como Norbert Wiener y Andrey Kolmogorov establecieron los cimientos teóricos en torno a los procesos estocásticos.\n\nLa relevancia de las series temporales en la predicción de datos radica en su habilidad para capturar patrones temporales y tendencias presentes en conjuntos de datos. A medida que se acumulan datos a lo largo del tiempo, es posible identificar relaciones y ciclos que facilitan la realización de pronósticos futuros. Esto resulta especialmente valioso en campos como la economía, la meteorología, la epidemiología y las finanzas, donde comprender los patrones temporales es crucial para tomar decisiones informadas.\n\nEn la actualidad, con el advenimiento de la computación y las técnicas de análisis más avanzadas, las series temporales han adquirido una importancia aún mayor. Modelos matemáticos y estadísticos avanzados, como los modelos ARIMA (Media Móvil Integrada Autoregresiva) y las redes neuronales recurrentes, permiten analizar y predecir series temporales con mayor precisión y complejidad. Estos modelos son esenciales para la toma de decisiones estratégicas en diversas industrias, ya que ayudan a anticipar tendencias, identificar patrones estacionales y enfrentar la incertidumbre en el futuro.\n\nEn términos generales, una serie temporal puede considerarse como una recopilación de observaciones realizadas secuencialmente en el tiempo. El interés de este análisis no recae en las series que son deterministas, sino en aquellas cuyos valores se comportan siguiendo las leyes de la probabilidad. Se discutirán los principios fundamentales involucrados en el análisis estadístico de series temporales. Para comenzar, se debe prestar una atención más meticulosa a la definición de serie temporal, dado que en realidad es un tipo particular de proceso estocástico.\n:::\n\n## Conceptos básicos y manipulación de series de tiempo\n\n::: {#def-ts style=\"text-align: justify\"}\n### Serie de tiempo\n\nUn proceso estocástico $X(t); t\\in T$ se define como una colección de variables aleatorias, donde $T$ es un conjunto de índices para el cual todas las variables aleatorias, $X(t)$, donde $t$ pertenece a $T$, están definidas en el mismo espacio muestral. Cuando $T$ representa el tiempo, se hace referencia al proceso estocástico como una *serie de tiempo*.\n:::\n\n::: {style=\"text-align: justify\"}\nSi $T$ toma un rango continuo de valores (por ejemplo, $T=(-\\infty,\\infty)$ o $T=(0,\\infty)$) , el proceso se dice que es un *proceso de parámetro continuo*. Si, por otro lado, $T$ toma un conjunto discreto de valores (por ejemplo, $T = \\{0, 1, 2,\\ldots\\}$ o $T = \\{0, \\pm 1, \\pm 2,\\ldots \\}$), el proceso se dice que es un *proceso de parámetro discreto*. De hecho, es común referirse a estos como [procesos continuos](procesos.qmd#proceso%20estocástico%20de%20tiempo%20discreto) y [discretos](procesos.qmd#proceso%20estocástico%20de%20tiempo%20continuo), respectivamente.\n\nSe utilizará la notación de subíndice, $X_t$, cuando se esté tratando específicamente con un proceso de parámetro discreto. Sin embargo, cuando el proceso involucrado sea de parámetro continuo o de tipo no especificado, se utilizará la notación de función, $X(t)$. Además, cuando no haya confusión, a menudo se utiliza la notación $\\{X(t)\\}$ o simplemente $X(t)$ para denotar una serie de tiempo. De manera similar, a menudo se acortará $\\{X_t;t=0,\\pm 1,\\ldots \\}$ a $X_t,t=\\{0,\\pm 1,\\ldots\\}$ o simplemente se usará $X_t$.\n\nNótese que una variable aleatoria, $\\gamma$ , es una función definida en un espacio muestral $\\Omega$ cuyo rango son los números reales. Un valor observado de la variable aleatoria $\\gamma$ es un número real $y=\\gamma(\\omega)$ para algún $\\omega\\in\\Omega$. Para una serie de tiempo $\\{X(t)\\}$, su \"valor\" $\\{X(t,\\omega);t\\in T\\}$ para algún $\\omega\\in\\Omega$ fijo es una colección de números reales. Esto lleva a la siguiente definición.\n:::\n\n::: {#def-realizacion style=\"text-align: justify\"}\n### Realización\n\nUna *realización* de la serie de tiempo $\\{X(t);t\\in T\\}$ es el conjunto de resultados de valores reales, $\\{X(t,\\omega);t\\in T\\}$ para un valor fijo de $\\omega \\in \\Omega$.\n:::\n\n::: {style=\"text-align: justify\"}\nLa colección de todas las posibles realizaciones se denomina *conjunto*, y, para un valor dado de $t$, la expectativa de la variable aleatoria $X(t)$ se denomina *media del conjunto* y se denotará como $\\mathrm E[X(t)]=\\mu(t)$ . La *varianza* de $X(t)$ se expresa como\n\n$$\n\\mathrm{Var}[X(t)]:=\\mathrm{E}[(X(t)-\\mu(t))^2]\n$$\n\ny a menudo se denota como $\\sigma^2(t)$ ya que también puede depender de $t$.\n:::\n\n::: {#.remark style=\"text-align: justify\"}\nDe especial interés en el análisis de una serie temporal es la covarianza entre $X(t_1)$ y $X(t_2)$, donde $t_1, t_2\\in T$. Dado que esta es la covarianza (@def-cov) dentro de la misma serie temporal, se refiere a ella como *autocovarianza*. De igual manera, se refiere a correlación (@def-corr) como *autocorrelación* y se denotan por\n\n$$\n\\gamma(t_1,t_2):=\\mathrm{E}[(X(t_1)-\\mu(t_1))(X(t_2)-\\mu(t_2))]\n$$ {#eq-autocov}\n\ny\n\n$$\n\\rho(t_1,t_2):= \\frac{\\gamma(t_1,t_2)}{\\sigma(t_1)\\sigma(t_2)} \n$$ {#eq-autocorr}\n\nrespectivamente.\n:::\n\n### Series de tiempo estacionarias\n\n::: {style=\"text-align: justify\"}\nEn el estudio de una serie de tiempo, es común que solo se tenga disponible una única realización de la serie. El análisis de una serie temporal basado únicamente en una realización es análogo a analizar las propiedades de una variable aleatoria en función de una sola observación. Los conceptos de estacionariedad y ergodicidad jugarán un papel importante en mejorar la capacidad de análisis de una serie temporal basada en una única realización de manera efectiva. Un proceso se considera estacionario si está en un estado de \"equilibrio estadístico\". El comportamiento básico de dicha serie de tiempo no cambia con el tiempo. Como ejemplo, para dicho proceso, $\\mu(t)$ no dependería del tiempo y, por lo tanto, podría ser denotado como $\\mu$ para todo $t$. Parecería que, dado que $x(t)$ para cada $t\\in T$ proporciona información sobre la media del conjunto, $\\mu$, podría ser posible estimar $\\mu$ en función de una única realización. Un proceso **ergódico** es aquel para el cual promedios de conjunto como $\\mu$ pueden estimarse consistentemente a partir de una sola realización. En esta sección, se presentarán definiciones más formales de estacionariedad. La noción más restrictiva de estacionariedad es la de estacionariedad estricta, que se define de la siguiente manera.\n:::\n\n::: {#def-PPE style=\"text-align: justify\"}\n#### Proceso estrictamente estacionario\n\nSe dice que un proceso $\\{X(t); t \\in T\\}$ es estrictamente estacionario si para cualquier $t_1, t_2,\\ldots, t_k \\in T$ y cualquier $h \\in T$, la distribución conjunta de $\\{X(t_1), X(t_2),\\ldots , X(t_k)\\}$ es idéntica a la de $\\{X(t_1 + h), X(t_2 + h),\\ldots, X(t_k + h)\\}$.\n:::\n\n::: {style=\"text-align: justify\"}\nLa estacionariedad estricta requiere, entre otras cosas, que para cualquier $t_1, t_2 \\in T$, las distribuciones de $X(t_1)$ y $X(t_2)$ deben ser las mismas, y además que todas las distribuciones bivariadas de pares $\\{X(t), X(t + h)\\}$ sean iguales para todos los $h$, etc. El requisito de estacionariedad estricta es riguroso y suele ser difícil de establecer matemáticamente. De hecho, para la mayoría de las aplicaciones, las distribuciones involucradas no se conocen. Por esta razón, se han desarrollado nociones menos restrictivas de estacionariedad. La más común de ellas es la estacionariedad por covarianza.\n:::\n\n::: {#def-estcov style=\"text-align: justify\"}\n#### Estacionariedad por covarianza\n\nLa serie de tiempo $\\{X(t); t \\in T\\}$ se considera estacionaria por covarianza si\n\ni.  $\\mu_{_{X_t}}=\\mathrm E[X(t)] = \\mu$ (media constante para todo $t$)\n\nii. $\\sigma^2_{_{X_t}}=\\mathrm{Var}[X(t)] = \\sigma^2 < \\infty$ (es decir, una constante finita para todo $t$)\n\niii. $\\gamma_{_{X_{t_1},X_{t_2}}}$ y $\\rho_{_{X_{t_1},X_{t_2}}}$ depende solo de $t_2 − t_1$.\n:::\n\n::: {style=\"text-align: justify\"}\nSi se cumple la condición iii., no habrá confusión al reemplazar la notación, $\\gamma_{_{X_{t_1},X_{t_2}}}$, con $\\gamma_{_{t_1-t_2}}$, y de manera similar, al denotar $\\rho_{_{X_{t_1},X_{t_2}}}$ como $\\rho_{_{t_1-t_2}}$. Cuando se establece $t_2-t_1=k$, se hace referencia a $\\gamma_{_k}$ y $\\rho_{_k}$ como la autocovarianza y la autocorrelación con un rezago de $k$, respectivamente.\n\nLa función de autocovarianza de una serie de tiempo estacionaria satisface las siguientes propiedades:\n\ni.  $\\gamma_{_0}=\\mathrm E[(X_{t}-\\mu)(X_{t}-\\mu)]=\\mathrm E[(X_{t}-\\mu)^2]=\\sigma^2$.\n\nii. $|\\gamma_{_k}|\\leq \\gamma_{_0}$ para todo $k$.\n\niii. $\\gamma_{_k}=\\mathrm{E}[(X_{t-k}-\\mu)(X_t-\\mu)]=\\mathrm{E}[(X_t-\\mu)(X_{t-k}-\\mu)]=\\gamma_{_{-k}}$.\n\niv. La función $\\gamma_{_k}$ es semidefinida positiva. Esto es, para cualquier conjunto de puntos de tiempo $t_1, t_2,\\ldots,t_k\\in T$ y para los reales $b_1,b_2,\\ldots, b_k$, se tiene\n\n    $$\\sum_{i=1}^k \\sum_{j=1}^k \\gamma(t_i-t_j)b_i b_j\\geq 0.\n    $$\n\nLa función de autocorrelación satisface las siguientes propiedades análogas:\n\ni.  $\\rho_{_0} = 1$.\n\nii. $|\\rho_{_k}|\\leq 1$ para todo $k$.\n\niii. $\\rho_{_k}=\\rho_{_{-k}}$.\n\niv. La función $\\rho_{_k}$ es semidefinida positiva, y para series de tiempo discretas definidas en $t = 0, 1, 2,\\ldots$, la matriz\n\n    $$\n    \\boldsymbol{\\rho}_k = \\begin{pmatrix}1 & \\rho_1 & \\ldots & \\rho_{_k}\\\\\n    \\rho_1 & 1 & \\ldots & \\rho_{k-1}\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    \\rho_{_k} & \\rho_{k+1}&\\ldots & 1\\end{pmatrix}\n    $$\n\n    es semidefinida positiva para cada $k$.\n:::\n\n::: {.remark style=\"text-align: justify\"}\nLa estacionariedad por covarianza también se conoce como estacionariedad débil, estacionariedad en el sentido amplio y estacionariedad de segundo orden. En el resto de esta tesis, a menos que se especifique lo contrario, el término estacionariedad se referirá a la estacionariedad por covarianza.\n:::\n\n::: {style=\"text-align: justify\"}\nEn las series de tiempo, al igual que en la mayoría de las otras áreas de la estadística, los datos no correlacionados desempeñan un papel importante. No hay dificultad en definir dicho proceso en el caso de una serie temporal de parámetro discreto. Es decir, la serie temporal $\\{X_t; t = 0, \\pm 1, \\pm 2,\\ldots\\}$ se llama \"proceso puramente aleatorio\" si los $X_t$ son variables aleatorias no correlacionadas. Al considerar procesos puramente aleatorios, solo nos interesará el caso en el que los $X_t$ también estén distribuidos de manera idéntica. En esta situación, es más común referirse a la serie de tiempo como ruido blanco. La siguiente definición resume estas observaciones.\n:::\n\n::: {#def-ruidob}\n#### Proceso de ruido blanco\n\nSe dice que un proceso $X_t$ es ruido blanco si se cumplen las siguientes condiciones.\n\n1.  Los $X_t$ están distribuidos de manera idéntica.\n\n2.  $\\gamma_{_{t_2-t_1}} = 0$ cuando $t_2 \\ne t_1$.\n\n3.  $\\gamma_{_{t-t}} = \\sigma^2$, donde $0 < \\sigma^2 < \\infty$ .\n:::\n\n::: {style=\"text-align: justify\"}\nEn un proceso de ruido blanco, cada observación está no correlacionada con todas las demás observaciones. Un hecho importante es que los procesos de ruido blanco son estacionarios.\n:::\n\n#### Estimación de los parámetros de un proceso estacionario.\n\n##### Estimación de $\\mu$.\n\n::: {style=\"text-align: justify\"}\nDada la realización $\\{x_t, t = 1, 2,\\ldots , n\\}$ de una serie temporal estacionaria, la estimación natural de la media común $\\mu$ es la media muestral.\n\n$$\n\\bar{x}=\\frac{1}{n}\\sum_{t=1}^n x_t\n$$ {#eq-media}\n\nEs evidente que el estimador $\\bar{X}$ es imparcial para $\\mu$.\n\nPara una serie temporal estacionaria, se puede emplear los datos a lo largo del tiempo para estimar la media, dado que se asume que la media es constante para cada instante de tiempo, $t$.\n:::\n\n###### Ergodicidad de $X$.\n\n::: {style=\"text-align: justify\"}\nSe dice que $X$ es ergódico para $\\mu$ si $X$ converge en el sentido de la media cuadrática hacia $\\mu$ a medida que $n$ aumenta, es decir, si $\\lim\\limits_{n\\to\\infty}\\mathrm{E}[(\\bar X -\\mu)^2]=0$.\n:::\n\n::: {#thm-ergo style=\"text-align: justify\"}\nSea $\\{X_t; t=0,\\pm 1,\\pm 2, \\ldots\\}$ una serie de tiempo estacionaria. Entonces, $\\bar X = \\frac{1}{n}\\sum\\limits_{t=1}^n X_t$ es ergódica para $\\mu$ si y solo si\n\n$$\n\\lim\\limits_{k\\to \\infty} \\frac{1}{k} \\sum_{j=0}^{k-1} \\gamma_j =0.\n$$ {#eq-ergo}\n\n::: proof\nVea @Yaglom1962introduction.\n:::\n:::\n\n::: {#cor-ergo style=\"text-align: justify\"}\nSea $X_t$ es una serie de tiempo estacionaria con parámetros discretos, como se establece en el @thm-ergo . Entonces, $\\bar X$ es ergódico para $\\mu$ si\n\n$$\n\\lim\\limits_{k\\to\\infty} \\gamma_{_k}=0,\n$$ {#eq-ergo1}\n\no equivalentemente si\n\n$$\n\\lim\\limits_{k\\to\\infty} \\rho_{_k}=0.\n$$ {#eq-ergo2}\n:::\n\n::: {style=\"text-align: justify\"}\nLa condición suficiente para la ergodicidad de $\\bar X$, dada en el @cor-ergo , resulta muy útil y es una condición que se cumple para la amplia clase de series temporales autorregresivas de media móvil, ARMA$(p,q)$ , estacionarias, que se discutirán más adelante. A pesar de que $X_t$'s \"cercanos\" en el tiempo pueden tener una correlación sustancial, la condición en el @cor-ergo asegura que para una \"gran\" separación, están casi no correlacionados.\n:::\n\n###### Varianza de $\\bar X$.\n\n::: {style=\"text-align: justify\"}\nAntes de abandonar el tema de estimar $\\mu$ a partir de una realización de un proceso estacionario, en el @thm-varmedia se proporciona una fórmula útil para la Varianza de $\\bar X$.\n\n::: {#thm-varmedia}\nSi $X_t$ es una serie de tiempo estacionaria, entonces la varianza de $\\bar X$ basada en una realización de longitud $n$ está dada por\n\n$$\n\\mathrm{Var}(\\bar X)=\\frac{\\sigma_X^2}{n}\\left(1+2\\sum_{k=1}^{n-1}\\left(1-\\frac{|k|}{n}\\right)\\rho_{_k}\\right)\n$$ {#eq-varmedia}\n:::\n:::\n\n::: {style=\"text-align: justify\"}\nEl resultado en la @eq-varmedia muestra el efecto de la autocorrelación en la varianza de $\\bar X$, y si $X_t$ es ruido blanco, es decir, $\\gamma_{_{k}} = 0$ si $k \\ne 0$, entonces la @eq-varmedia se convierte en el conocido resultado $Var(\\bar X) = \\sigma^2/n$.\n\nUtilizando la notación $\\hat \\rho_{_k}$ para denotar las autocorrelaciones estimadas (muestra) y $\\hat\\sigma^2=\\hat{\\gamma_0}$ para denotar la varianza muestral, es práctica común obtener intervalos de confianza aproximados del $95\\%$ para $\\mu$ usando\n\n$$\n\\left(\\bar X- 1.96\\sqrt{\\frac{\\hat\\sigma^2}{n}\\sum_{k=-(n-1)}^{n-1}\\left(1-\\frac{|k|}{n}\\right)\\hat\\rho_{_k}}, \\bar X+ 1.96\\sqrt{\\frac{\\hat\\sigma^2}{n}\\sum_{k=-(n-1)}^{n-1}\\left(1-\\frac{|k|}{n}\\right)\\hat\\rho_{_k}}\\right).\n$$ {#eq-iconf}\n:::\n\n##### Estimación de $\\gamma_{_k}$.\n\n::: {style=\"text-align: justify\"}\nDebido a la estacionariedad, $\\mathrm E[(X_t − \\mu)(X_{t+k} − \\mu)] = \\gamma_{_k}$ no depende de $t.$ Como consecuencia, parece razonable estimar $\\gamma_{_k}$ a partir de una sola realización, por\n\n$$\n\\begin{split}\\hat{\\gamma}_{_k}&= \\frac{1}{n}\\sum_{t=1}^{n-k}(X_t-\\bar X)(X_{t+k}-\\bar X), \\quad 0\\leq k\\leq n\\\\\n&=0,\\qquad k\\ge n\\\\\n&=\\hat{\\gamma}_{_{-k}},\\quad k<0\\end{split}\n$$ {#eq-estgamma}\n:::\n\n::: {.remark style=\"text-align: justify\"}\nUsando @eq-estgamma se deduce que\n\n$$\n\\hat{\\gamma}_{_0}= \\frac{1}{n}\\sum_{t=1}^{n}(X_t-\\bar X)^2\n$$ {#eq-hatgamma0}\n:::\n\n##### Estimación de $\\rho_{_k}$.\n\n::: {#def-samplecorr style=\"text-align: justify\"}\n##### Autocorrelación muestral\n\nEl estimador de la autocorrelación, $\\rho_{_{k}}$ se obtiene mediante\n\n$$\n\\hat\\rho_{_{k}}=\\hat\\gamma_{_{k}}/\\hat\\gamma_{_{0}}\n$$ {#eq-sampleautocor}\n\nA este estimador se le conoce como la autocorrelación muestral.\n:::\n\n::: {style=\"text-align: justify\"}\nA partir del examen de la @eq-estgamma , es evidente que los valores de $\\hat\\gamma_{_{k}}$ (y $\\hat\\rho_{_{k}}$) tenderán a ser \"pequeños\" cuando $k$ sea grande en relación con $n$.\n:::\n\n### Conjuntos de datos de series temporales.\n\n::: {style=\"text-align: justify\"}\nLos comportamientos exhibidos por los datos de series temporales son diversos y se analizarán en las secciones subsiguientes. Dichos tipos de comportamiento se ilustrarán mediante ejemplos provenientes de la realidad, tales como los datos intrigantes de manchas solares, registros de temperatura, el índice Dow Jones, precios de acciones individuales, datos de ventas (tanto mensuales como diarios), entre otros. El análisis comenzará con una discusión sobre datos que presenten algún tipo de patrón cíclico (@woodward2022time).\n:::\n\n#### Datos Cíclicos\n\n::: {style=\"text-align: justify\"}\nMuchos conjuntos de datos de series temporales muestran un patrón cíclico, lo que significa que los datos presentan aumentos y disminuciones de manera algo repetitiva. Estos datos a veces se denominan *\"pseudo-periódicos\"*, un término que usaremos de manera sinónima con *\"cíclico\"*. Los datos de manchas solares en la @fig-sunspot es un ejemplo de datos cíclicos.\n\n::: remark\nLos datos verdaderamente periódicos exhiben un comportamiento que se replica de manera precisa a lo largo de un período de tiempo establecido. Un caso ilustrativo de datos puramente periódicos se encuentra en la forma de la curva sinusoidal. De este modo, los datos pseudoperiódicos (o cíclicos) se refieren a aquellos conjuntos de datos que tienden a mostrar repeticiones en sus comportamientos.\n:::\n:::\n\n::: {#exm-sunspot style=\"text-align: justify\"}\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n##### Datos de manchas solares\n\nLa @fig-sunspot muestra datos anuales de manchas solares para los años 1700-2020. Las manchas solares son áreas de explosiones solares o disturbios atmosféricos extremos en el sol. En 1848, el astrónomo suizo Rudolf Wolf introdujo un método para contar la actividad de las manchas solares, y los datos mensuales utilizando su método están disponibles desde 1749. (@waldmeier1961sunspot).\n\n::: {#fig-sunspot}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tswge)\nplot(sunspot2.0, xlab='Año', ylab='Manchas solares')\n```\n\n::: {.cell-output-display}\n![](series_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNúmero anual de manchas solares desde 1700 hasta 2020.\n:::\n\nLas manchas solares han generado un considerable interés en la comunidad científica por dos razones principales:\n\n-   La actividad de las manchas solares tiende a afectarnos aquí en la Tierra. Por ejemplo, una alta actividad de manchas solares provoca interferencias en la comunicación por radio y se asocia con una mayor intensidad de luz ultravioleta y actividad de auroras boreales.\n\n-   La actividad de las manchas solares tiene un comportamiento cíclico que tiene una duración de ciclo de aproximadamente 11 años. Al examinar la @fig-sunspot se observa que hay 29 ciclos en los 321 años, con una duración media del ciclo de aproximadamente 11 años. De hecho, las duraciones de los ciclos tienden a variar aleatoriamente entre 9 y 13 años.\n\nSi bien el comportamiento cíclico en la @fig-sunspot es claro, a menudo es útil examinar fragmentos cortos de los datos para visualizar mejor el comportamiento específico. La @fig-sunsfrag muestra el número de manchas solares desde 1867 hasta 1950. Las líneas verticales identifican los años en los que hubo un pico en los números de manchas solares y las flechas horizontales representan el tiempo entre los picos. Para los años representados en la @fig-sunsfrag, las duraciones de los ciclos fueron de 13, 10, 12, 12, 11, 9 y 10 años, respectivamente. Las duraciones de los ciclos parecen variar aleatoriamente y no parece haber un \"ajuste a una duración de ciclo fija\". De hecho, según la comprensión de estos autores, los científicos no tienen una explicación física para el ciclo de aproximadamente 11 años. Los datos de las manchas solares son un ejemplo clásico de datos cíclicos con duraciones de ciclo variables. De hecho, @yule1971method desarrolló el proceso autorregresivo como un medio para describir el comportamiento periódico \"perturbado\" de los datos de las manchas solares.\n\n::: {#fig-sunsfrag}\n\n::: {.cell}\n::: {.cell-output-display}\n![](series_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nFragmento de la @fig-sunspot que muestra los años 1867-1950\n:::\n:::\n:::\n\n::: {#exm-airpass style=\"text-align: justify\"}\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n##### Datos de pasajeros aéreos\n\nLa @fig-airpass es un conjunto de datos que contiene el número total (en miles) de pasajeros de líneas aéreas internacionales por mes durante los 12 años, desde 1949 hasta 1960. Estos datos han sido analizados exhaustivamente y son un conjunto de datos clásico en la literatura de series temporales. Los datos siguen un patrón cíclico de 12 meses que es similar de un año a otro y está basado en el año calendario. Por lo tanto, los datos de Pasajeros Aéreos son otro ejemplo de datos estacionales. Además, los datos tienden a mostrar una tendencia al alza con el tiempo. Es decir, el número de pasajeros de líneas aéreas está aumentando con el tiempo. El comportamiento de tendencia en series temporales se discutirá en la @sec-trend. También hay una variabilidad creciente dentro del año. Este tipo de comportamiento, conocido como *estacionalidad multiplicativa*.\n\n::: {#fig-airpass}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tswge)\nplot(AirPassengers, xlab='Año', ylab='Número de pasajeros')\n```\n\n::: {.cell-output-display}\n![](series_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nNúmero de Pasajeros Internacionales en Aerolíneas de 1949 a 1960\n:::\n\nLa @fig-airfrag muestra un fragmento de los datos de pasajeros aéreos desde 1957 hasta 1960. Se observa que el viaje aéreo es ligero desde Enero hasta Abril, aumenta durante los meses de verano y comienza a disminuir en septiembre hasta noviembre con un ligero aumento en diciembre. Este patrón, aunque no es sinusoidal, se repite de un año a otro. El comportamiento cíclico de los datos de pasajeros aéreos se repite anualmente y es un ejemplo de datos estacionales que no son seudosenoidales.\n\n::: {#fig-airfrag}\n\n::: {.cell}\n::: {.cell-output-display}\n![](series_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nFragmento de la @fig-airpass que muestra los años 1957-1960\n:::\n:::\n:::\n\n#### Tendencias {#sec-trend}\n\n::: {style=\"text-align: justify\"}\nUna tendencia en un contexto de análisis de datos se refiere a la inclinación de una serie de datos a experimentar un incremento o disminución constante a lo largo del tiempo. En el caso específico de los datos de Pasajeros Aéreos mostrados en la @fig-airpass, se identifica un patrón de crecimiento además del patrón estacional previamente observado. Una tendencia lineal se caracteriza por el aumento o la disminución de los datos de manera constante y progresiva, tal como se ilustra en la @fig-lineal. Las tendencias pueden seguir una curva, como lo ejemplifica la tendencia exponencial en la @fig-expo. Por otro lado, la @fig-desc exhibe una serie temporal con una tendencia descendente, pero su naturaleza es más irregular en comparación con las tendencias representadas en las Figuras (a) y (b). Un patrón común en conjuntos de datos es un comportamiento de tendencia aleatoria, como se muestra en la @fig-deam, la cual sugiere una trayectoria sin un rumbo definido. Esto implica que pueden existir tendencias de corta o larga duración, en ocasiones en direcciones opuestas.\n:::\n\n::: {#fig-trends layout=\"[[1,1], [1,1]]\"}\n![Tendencia lineal](Imagenes/lineal.png){#fig-lineal}\n\n![Tendencia exponencial](Imagenes/exponencial.png){#fig-expo}\n\n![Tendencia descendente](Imagenes/abajo.png){#fig-desc}\n\n![Comportamiento deambulante](Imagenes/deambulante.png){#fig-deam}\n\nGráficos que muestran (a) una tendencia lineal, (b) una tendencia exponencial, (c) una tendencia descendente irregular y (d) un patrón de deambulación.\n:::\n\n::: {#def-aper style=\"text-align: justify\"}\nLa función $g(t)$ es *periódica* con periodo (o longitud del ciclo) $p > 0$ si $p$ es el valor más pequeño tal que $g(t) = g(t + kp)$ para todo $t$ y enteros $k$. Se dice que una función $g(t)$ es *aperiódica* si no existe tal $p$.\n:::\n\n::: {#def-freq style=\"text-align: justify\"}\n#### Frecuencia\n\nLa frecuencia, denotada por $f$, puede ser descrita de las siguientes dos maneras;\n\n1.  $f=1/\\text{periodo}$ (tamaño del ciclo)\n\n2.  El número de ciclos en la función a través de una unidad de tiempo.\n:::\n\n::: {.remark style=\"text-align: justify\"}\nLos datos con comportamiento de tendencia y deambulación aleatoria no son cíclicos por naturaleza. A veces se les llama aperiódicos debido a la ausencia de un comportamiento regular de ascenso y descenso.\n:::\n\n#### Definición y propiedades del espectro y densidad espectral\n\n::: {#def-spec style=\"text-align: justify\"}\nSea $X_{_t}$ una serie de tiempo estacionaria con autocovarianza $\\gamma_{_k}$ y autocorrelación $\\rho_{_k}$. Entonces para $|f|\\le 0.5$:\n\ni.  El *espectro* de $X_{_t}$ se define como\n\n    $$\n    P_{_X}(f)=\\sum_{k=-\\infty}^\\infty e^{-2\\pi ifk}\\gamma_{_k}.\n    $$ {#eq-spectrum}\n\nii. La *densidad espectral* de $X_{_t}$ se define como\n\n    $$\n    S_{_X}(f)=\\sum_{k=-\\infty}^\\infty e^{-2\\pi ifk}\\rho_{_k}.\n    $$ {#eq-spectden}\n:::\n\n::: {style=\"text-aling: justify\"}\nUsando la fórmula de Euler, se obtienen las fórmulas \"más agradables\".\n\n$$\nP_{_X}(f)=\\sigma_{_X}^2+2\\sum_{k=1}^\\infty\\gamma_{_k}\\cos(2\\pi fk)\n$$\n\nY\n\n$$\nS_{_X}(f)=1+2\\sum_{k=1}^\\infty \\rho_{_k}\\cos(2\\pi fk)\n$$\n\nEstas fórmulas enfatizan que el espectro y la densidad espectral son funciones de valor real, lo que no es evidente en @eq-spectrum y @eq-spectden.\n\n***Propiedades importantes de densidades espectrales***\n\ni.  $S_{_X}(f)\\geq 0$.\n\nii. $S_{_X}(f)=S_{_X}(-f)$.\n\niii. $S_{_X}(f)=1+2\\sum\\limits_{k=1}^\\infty \\rho_{_k}\\cos(2\\pi fk)$, donde $|f|\\le 0.5$ .\n\niv. $\\sum\\limits_{-0.5}^{0.5}S_{_X}(f)e^{2\\pi ifk}df=\\rho_{_k}$\n\n    Las propiedades i y ii muestran que $S_{_X}(f)$ es una función par no negativa.\n:::\n\n### Suavizado de datos de series temporales.\n\n::: {style=\"text-align: justify\"}\nExisten varios métodos para \"suavizar\" el comportamiento ruidoso (posiblemente poco importante) de una serie temporal para que se pueda entender mejor una señal importante subyacente. Se comienza discutiendo el método de suavizado de promedio móvil centrado, que es el más básico.\n:::\n\n#### Suavizado de datos utilizando un suavizador de promedio móvil centrado {#sec-mas}\n\n::: {style=\"text-align: justify\"}\nEl suavizador de promedio móvil centrado es un método para reemplazar los valores de datos en una serie temporal con un promedio de los valores de datos que rodean (e incluyen) ese punto de datos. Por ejemplo, un suavizador de promedio móvil centrado de orden tres reemplaza un valor de datos $x_{_t}$ en el tiempo $t$ con $s_{_t} = (x_{_{t-1}}+x_{_t}+x_{_{t+1}})/3$. Es decir, se asigna el valor promedio al punto de tiempo medio. Por lo tanto, un suavizador de promedio móvil centrado de orden tres no puede asignarse al primer o último punto de tiempo de una serie temporal. Se sigue que a mayor orden, más valores faltarán al principio y al final del conjunto de datos suavizado. Para un promedio móvil centrado de tercer orden, la fórmula de promediado se desplaza a lo largo del conjunto de datos de la serie temporal, considerando tres valores de datos consecutivos juntos hasta llegar a los últimos tres puntos temporales.\n:::\n\n::: {#def-CMAS style=\"text-align: justify\"}\n##### Suavizador de Promedio Móvil Centrado\n\nSea $x_{_t}, t=1,\\ldots,n$ un conjunto de datos de series temporales. El suavizador de promedio móvil centrado se define de la siguiente manera:\n\n***Caso 1:*** $m$ ***es un número impar***\n\nSea $k= (m−1)/2$. Para $k <t< n - k$, el valor de los datos suavizados, $s_{_t}$, en el tiempo $t$ se da por\n\n$$\ns_{_t}=\\frac{1}{m}\\sum_{i=t-k}^{t+k}x_{_i}\n$$ {#eq-mimpar}\n\n***Caso 2:*** $m$ ***es un número par***\n\nSea $k= m/2$ Para $k <t< n - k$, el valor de los datos suavizados, $s_{_t}$, en el tiempo $t$ se da por\n\n$$\ns_{_t}= \\frac{x_{_{t-k}}}{2m}+\\frac{1}{m}\\sum_{i=t-k+1}^{t+k-1}x_{_i}+\\frac{x_{_{t+k}}}{2m}\n$$ {#eq-mpar}\n:::\n\n::: {.callout-caution collapse=\"true\" style=\"text-align: justify\" icon=\"false\"}\n## Ejemplos\n\n-   Para un promedio móvil centrado de quinto orden en tiempos $2 <t < n -2$ , se tiene\n\n    $$\n    s_{_t}=(x_{_{t-2}}+x_{_{t-1}}+x_{_t}+x_{_{t+1}}+x_{_{t+2}})/5.\n    $$\n\n-   El suavizador de promedio móvil de cuarto orden en tiempos $2 <t < n -2$ , está dado por\n\n    $$\n    s_{_t}=\\frac{x_{_{t-2}}}{8}+\\frac{x_{_{t-1}}+x_{_t}+x_{_{t+1}}}{4}+\\frac{x_{_{t+2}}}{8}\n    $$\n\nEl suavizador de promedio móvil centrado tiene dos usos básicos:\n\n-   Suavizado diseñado para eliminar fluctuaciones (potencialmente sin sentido) de los datos.\n\n-   Eliminar el comportamiento cíclico de datos estacionales u otros datos cíclicos con longitudes de ciclo fijas.\n:::\n\n::: {style=\"text-align: justify\"}\nEl @exm-ejem muestra el uso del suavizado de promedio móvil centrado con el propósito de detectar o comprender mejor señales subyacentes y fundamentales en los datos.\n\n::: {#exm-ejem style=\"text-align: justify\"}\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n##### Suavizando los datos de temperatura de Tesla y DFW.\n\nLos precios de las acciones de Tesla desde el 1 de enero de 2020 hasta el 30 de abril de 2021 se muestran en la @fig-charts-1 . Se observa el hecho de que hubo un aumento constante hasta principios de 2021, momento en el cual el precio se estabilizó y disminuyó. Sin embargo, hay una considerable fluctuación de un día para otro, especialmente en 2021. La @fig-charts-4 muestra las temperaturas medias anuales de DFW (Dallas Ft. Worth) desde 1900 hasta 2020. Allí se observa una considerable fluctuación de un año a otro, pero algo de aumento a partir de aproximadamente 2000. La @fig-charts-2, (c), (e) y (f) muestran versiones suavizadas de las Figuras @fig-charts-1 y (d). Al utilizar el suavizador de promedio móvil centrado, las fluctuaciones de un día para otro se suavizan; se nota que el orden 8 produce más suavizado que el orden 3. El comportamiento fundamental, incluida la estabilización y disminución a principios de 2021, se ve con más claridad al minimizar los cambios ruidosos de un día para otro. El efecto del suavizado es más evidente en los datos de temperatura de DFW. La fluctuación de un año a otro es bastante dramática en el conjunto de datos original en la @fig-charts-4 . Un suavizado de orden 3 proporciona cierta claridad, pero el suavizado de orden 8 mostrado en la @fig-charts-5 muestra claramente un comportamiento casi estable desde 1900 hasta aproximadamente 1985. Desde entonces, ha habido un aumento que puede haberse estabilizado o no en los últimos años. Es particularmente notable en la @fig-charts-5 que el suavizado ha eliminado los extremos. Específicamente, las temperaturas extremadamente altas en 2012, 2016 y 2017 se ven moderadas por las temperaturas más bajas en los años circundantes. (Tomado de @woodward2022time)\n\n\n::: {#fig-charts .cell layout-nrow=\"2\" layout-ncol=\"3\"}\n::: {.cell-output-display}\n![Precios de las acciones de Tesla](series_files/figure-html/fig-charts-1.png){#fig-charts-1 width=672}\n:::\n\n::: {.cell-output-display}\n![MA Smoother Tesla: Orden=3](series_files/figure-html/fig-charts-2.png){#fig-charts-2 width=672}\n:::\n\n::: {.cell-output-display}\n![MA Smoother Tesla: Orden=8](series_files/figure-html/fig-charts-3.png){#fig-charts-3 width=672}\n:::\n\n::: {.cell-output-display}\n![Temperatura anual DFW](series_files/figure-html/fig-charts-4.png){#fig-charts-4 width=672}\n:::\n\n::: {.cell-output-display}\n![MA Smoother DFW: Orden=3](series_files/figure-html/fig-charts-5.png){#fig-charts-5 width=672}\n:::\n\n::: {.cell-output-display}\n![MA Smoother DFW: Orden=8](series_files/figure-html/fig-charts-6.png){#fig-charts-6 width=672}\n:::\n\nPrecios de las acciones de Tesla y datos de temperatura anual de DFW antes y después de aplicar suavizadores de promedio móvil de orden 3 y 8\n:::\n\n:::\n:::\n:::\n\n## Análisis y técnicas de descomposición.\n\n### Descomposición de datos estacionales\n\n::: {style=\"text-align: justify\"}\nEn el @exm-airpass se aborda la naturaleza de los datos estacionales, entendidos como una serie de datos cíclicos con periodos consistentes y un patrón que guarda relación con el calendario. El conjunto de datos de AirPassengers presentado en la @fig-airpass se clasifica como un ejemplo paradigmático de datos estacionales. Este conjunto de datos exhibe un comportamiento estacional anual, además de una tendencia de crecimiento, que se aproxima a ser lineal. Es convencional considerar que los datos estacionales, denotados como $x_{_t}$, comprenden:\n\na.  Un componente estacional intrínseco anual, identificado como $s_{_t}$,\n\nb.  Un componente de tendencia a largo plazo, referido como $tr_{_t}$, y\n\nc.  Un componente de variabilidad aleatoria, conocido como $z_{_t}$.\n\nSe ha observado esta estructura en el conjuntos de datos ya mencionado. Los expertos en análisis de series temporales se enfocan en dos categorías de modelos estacionales:\n\n*Datos estacionales aditivos*\n\nLos datos $x_{_t}$, en el tiempo $t$ pueden ser consideramos como una suma dada en la @eq-addseas\n\n$$\nx_{_t}=s_{_t}+tr_{_t}+z_{_t}.\n$$ {#eq-addseas}\n\n*Datos estacionales multiplicativos*\n\nLos datos, $x_{_t}$, en el tiempo $t$ pueden ser expresados como el producto dado en la @eq-mulseas\n\n$$\nx_{_t}=s_{_t}\\times tr_{_t}\\times z_{_t}.\n$$ {#eq-mulseas}\n:::\n\n::: {#exm-air style=\"text-align: justify\"}\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n#### Datos de pasajeros aéreos\n\nPara ilustrar la diferencia entre los tipos de datos que se ajustan mejor a un modelo aditivo y a uno multiplicativo, se utiliza el conjunto de datos de AirPassengers. Como se ha señalado anteriormente, los datos de AirPassengers, representados en la @fig-air-1, tienen un componente estacional y de tendencia, pero también la variabilidad dentro del año aumenta con el tiempo.\n\n\n::: {#fig-air .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nlibrary(tswge)\ndata(AirPassengers)\nlogAirPassengers=log(AirPassengers)\nplot(AirPassengers)\nplot(logAirPassengers)\n```\n\n::: {.cell-output-display}\n![Datos de pasajeros aéreos: 1949-1960](series_files/figure-html/fig-air-1.png){#fig-air-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Datos de pasajeros aéreos en escala logarítmica](series_files/figure-html/fig-air-2.png){#fig-air-2 width=672}\n:::\n\nDatos de pasajeros aereos y logaritmo de los datos de pasajeros aereos\n:::\n\n\nLos conjuntos de datos con este tipo de comportamiento suelen modelarse utilizando modelos multiplicativos. Para eliminar el aumento en la variabilidad, los analistas suelen tomar el logaritmo de los datos y utilizan los \"datos logarítmicos\" para el análisis.\n\nLos datos logarítmicos de logAirPassengers en la @fig-air-2 no muestran un aumento en la variabilidad dentro del año y son un ejemplo clásico de datos que se modelan utilizando el modelo aditivo en la @eq-addseas.\n:::\n:::\n\nSe comenzó discutiendo el modelo aditivo, considerado el más intuitivo de los dos.\n\n::: {style=\"text-align: justify\"}\nA continuación, se discutirán las diferencias en las estrategias de modelado para estos dos conjuntos de datos.\n\nLas descomposiciones aditivas y multiplicativas siguen pasos de implementación similares:\n\n1.  Estimar el componente de tendencia.\n2.  Eliminar el componente de tendencia, lo que resulta en un conjunto de datos compuesto principalmente por las fluctuaciones estacionales en los datos.\n3.  Calcular un componente estacional \"promedio\" dentro del año.\n4.  Encontrar el ruido restante.\n\nSe comenzará discutiendo el modelo aditivo, que es el más intuitivo de los dos.\n:::\n\n#### Descomposición aditiva\n\n::: {style=\"text-align: justify\"}\nCuando se analizan datos utilizando el modelo aditivo en la @eq-addseas, se parte del supuesto de que los datos son la suma de componentes estacionales, de tendencia y de ruido aleatorio. Se discuten los pasos de análisis involucrados en la descomposición de los datos logarítmicos de logAirPassengers. En la práctica, los componentes en @eq-addseas se estiman y un modelo estimado puede describirse como\n\n$$\nx_{_t}=\\hat{s_{_t}}+\\hat{tr_{_t}}+\\hat{z_{_t}}\n$$ {#eq-addecompose}\n\n::: {#exm-logair}\n::: {.callout-caution collapse=\"true\" style=\"text-align: justify\" icon=\"false\"}\n##### Descomposición aditiva de LogAirPassengers\n\nLa descomposición de los datos logAirPassengers se logra mediante los siguientes pasos.\n\na.  ***Estimar el Componente de Tendencia***: La @fig-sm12 es una representación gráfica del conjunto de datos logAirPassengers superpuesto con el resultado de aplicar un suavizador de media móvil centrada de orden 12 a los datos.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(tswge)\n    data(AirPassengers)\n    logAirPassengers=log(AirPassengers)\n    logair.12=ma.smooth.wge(logAirPassengers,order=12)\n    logair.12$smooth\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n      [1]       NA       NA       NA       NA       NA       NA 4.837280 4.841114\n      [9] 4.846596 4.851238 4.854488 4.859954 4.869840 4.881389 4.893411 4.904293\n     [17] 4.912752 4.923701 4.940483 4.957406 4.974380 4.991942 5.013095 5.033804\n     [25] 5.047776 5.060902 5.073812 5.088378 5.106906 5.124312 5.138282 5.152751\n     [33] 5.163718 5.171454 5.178401 5.189431 5.203909 5.218093 5.231553 5.243722\n     [41] 5.257413 5.270736 5.282916 5.292150 5.304079 5.323338 5.343560 5.357427\n     [49] 5.367695 5.378309 5.388417 5.397805 5.403849 5.407220 5.410364 5.410294\n     [57] 5.408381 5.406761 5.406218 5.410571 5.419628 5.428330 5.435128 5.442237\n     [65] 5.450659 5.461103 5.473655 5.489713 5.503974 5.516367 5.529403 5.542725\n     [73] 5.557864 5.572693 5.587498 5.602730 5.616658 5.631189 5.645937 5.659812\n     [81] 5.674172 5.687636 5.700766 5.714738 5.727153 5.738856 5.750676 5.760658\n     [89] 5.770846 5.780430 5.788745 5.796524 5.804821 5.814072 5.823075 5.832692\n     [97] 5.842665 5.853541 5.864863 5.875490 5.885654 5.894475 5.901555 5.907026\n    [105] 5.910012 5.910708 5.911637 5.913829 5.917360 5.922887 5.926146 5.927563\n    [113] 5.929657 5.930458 5.932964 5.938377 5.946188 5.956352 5.967813 5.977291\n    [121] 5.985269 5.994078 6.003991 6.014899 6.026589 6.040709 6.054492 6.066195\n    [129] 6.073088 6.080733 6.091930 6.102013 6.112511 6.121153 6.128381 6.137437\n    [137] 6.145733 6.151526       NA       NA       NA       NA       NA       NA\n    ```\n    \n    \n    :::\n    \n    ```{.r .cell-code}\n    logair.sm12=ts(logair.12$smooth,start=c(1949,1),frequency=12)\n    ```\n    \n    ::: {.cell-output-display}\n    ![LogAirPassengers con un suavizador de promedio móvil centrado de orden 12.](series_files/figure-html/fig-sm12-1.png){#fig-sm12 width=672}\n    :::\n    :::\n\n\n    En relación con el modelo estimado en @eq-addecompose, $\\hat{tr_{_t}} =$ **logair.sm12** representa la curva casi lineal en la @fig-sm12.\n\nb.  ***Eliminar el Componente de Tendencia de los Datos:*** El paso subsiguiente implica la sustracción del componente de tendencia estimado de los datos (logAirPassengers).\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    seas.logair=logAirPassengers-logair.sm12\n    round(seas.logair,4) \n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n             Jan     Feb     Mar     Apr     May     Jun     Jul     Aug     Sep\n    1949      NA      NA      NA      NA      NA      NA  0.1599  0.1561  0.0661\n    1950 -0.1249 -0.0451  0.0553  0.0010 -0.0844  0.0802  0.1953  0.1784  0.0882\n    1951 -0.0710 -0.0503  0.1080  0.0054  0.0406  0.0575  0.1550  0.1406  0.0512\n    1952 -0.0622 -0.0251  0.0311 -0.0452 -0.0479  0.1138  0.1552  0.1968  0.0383\n    1953 -0.0896 -0.1002  0.0754  0.0618  0.0299  0.0858  0.1656  0.1955  0.0597\n    1954 -0.1015 -0.1919  0.0245 -0.0173  0.0047  0.1148  0.2368  0.1905  0.0529\n    1955 -0.0689 -0.1217 -0.0002 -0.0080 -0.0182  0.1214  0.2512  0.1895  0.0688\n    1956 -0.0782 -0.1148  0.0082 -0.0145 -0.0088  0.1438  0.2347  0.2074  0.0673\n    1957 -0.0901 -0.1464  0.0101 -0.0233 -0.0135  0.1505  0.2405  0.2393  0.0914\n    1958 -0.0884 -0.1608 -0.0345 -0.0754 -0.0353  0.1449  0.2635  0.2862  0.0552\n    1959 -0.0992 -0.1593  0.0024 -0.0335  0.0137  0.1163  0.2518  0.2600  0.0646\n    1960 -0.0794 -0.1524 -0.0905 -0.0040  0.0112  0.1307      NA      NA      NA\n             Oct     Nov     Dec\n    1949 -0.0721 -0.2101 -0.0893\n    1950 -0.1016 -0.2769 -0.0922\n    1951 -0.0839 -0.1948 -0.0774\n    1952 -0.0711 -0.1961 -0.0896\n    1953 -0.0549 -0.2133 -0.1073\n    1954 -0.0826 -0.2162 -0.1090\n    1955 -0.0745 -0.2327 -0.0871\n    1956 -0.0905 -0.2210 -0.1091\n    1957 -0.0614 -0.1913 -0.0967\n    1958 -0.0730 -0.2312 -0.1572\n    1959 -0.0719 -0.2003 -0.0981\n    1960      NA      NA      NA\n    ```\n    \n    \n    :::\n    :::\n\n    \n\n::: {#fig-seasair .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![LogAirPassengers sin tendencia](series_files/figure-html/fig-seasair-1.png){#fig-seasair-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Componente estacional estimado](series_files/figure-html/fig-seasair-2.png){#fig-seasair-2 width=672}\n:::\n\nDatos de LogAirPassengers sin el componente de tendencia y componente estacional estimado\n:::\n\n\nSe aprecia con mayor claridad el comportamiento estacional año tras año en la @fig-seasair-1 sin la \"interferencia\" de la tendencia. Específicamente, se observa un patrón similar en cada año (mayor cantidad de viajes en verano, disminución en noviembre, aún bajos pero con una ligera alza en diciembre, continuamente bajos en enero y febrero, alza en marzo, y así sucesivamente). No obstante, se presentan variaciones de un año a otro: los viajes aéreos en noviembre fueron notablemente bajos en 1950 y luego inusualmente altos en julio y agosto de 1958.\n\nc.  ***Calcular un \"Promedio\" del Componente Estacional Dentro del Año:*** Es importante notar que el componente estacional en el modelo (2.4) es un patrón general que se mantiene igual de un año a otro. Es decir,$\\{s_{_t}, t=1,\\ldots,12\\} = \\{s_{_{t+12}},t=1,\\ldots,12\\} =\\{s_{_{t+2(12)}}, t=1,\\ldots,12\\}=\\cdots.$ El componente de ruido, $z_{_t}$, ajusta las variaciones de un año a otro del patrón estacional general. El patrón estacional, $s_{_t}, t=1,\\ldots,12$, se estima calculando el promedio a lo largo de los años, y el componente estacional estimado, $\\hat{s}_{_t}$ (el cual es el mismo para cada año), se muestra en la @fig-seasair-2.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    seas.logair.numeric=as.numeric(seas.logair)\n    seas.logair.matrix=matrix(seas.logair.numeric,ncol=12)\n    seas.logair.matrix.t=t(seas.logair.matrix)\n    months=colMeans(seas.logair.matrix.t, na.rm=TRUE)\n    round(months,4)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n     [1] -0.0867 -0.1153  0.0172 -0.0139 -0.0098  0.1145  0.2100  0.2036  0.0640\n    [10] -0.0761 -0.2167 -0.1012\n    ```\n    \n    \n    :::\n    \n    ```{.r .cell-code}\n    seas.means=rep(months,12)\n    seas.means=ts(seas.means,start=c(1949,1),frequency=12)\n    ```\n    :::\n\n\nd.  ***Encontrar el componente de ruido restante:*** El ruido estimado en @eq-addecompose, $\\hat{z_{_t}}$, es calculado de la siguiente manera\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    logair.noise = logAirPassengers - logair.sm12 - seas.means\n    plot(decompose(logAirPassengers))\n    ```\n    \n    ::: {.cell-output-display}\n    ![Descomposición aditiva de LogAirPassengers](series_files/figure-html/fig-descadd-1.png){#fig-descadd width=672}\n    :::\n    :::\n\n\nLa @fig-descadd muestra un gráfico de los datos de LogAirPassengers junto con las partes del procedimiento de descomposición.\n:::\n:::\n:::\n\n#### Descomposición multiplicativa\n\n::: {style=\"text-align: justify\"}\nSe llevará a cabo en el @exm-mulair una descomposición multiplicativa de los datos de AirPassengers. Se destaca que esta serie temporal exhibe un patrón estacional y una variabilidad intra-anual crecientes con el tiempo. A pesar de la posibilidad de modelar estos datos mediante el uso del logaritmo seguido de un modelo aditivo, en esta sección se opta por un enfoque multiplicativo para analizar los datos originales de AirPassengers. Al emplear la descomposición multiplicativa en el análisis de datos, se hace la suposición de que la serie temporal es el resultado de componentes estacionales, de tendencia y de ruido. El modelo estimado se expresa como;\n\n$$\nx_{_t}=\\hat{s_{_t}}\\times \\hat{tr_{_t}}\\times \\hat{z_{_t}}.\n$$ {#eq-muldecom}\n\n::: {#exm-mulair}\n<div>\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n##### Descomposición multiplicativa de AirPassengers\n\na.  ***Estimar el Componente de Tendencia***: Al igual que con el modelo aditivo, el primer paso consiste en utilizar un suavizador de media móvil centrada, nuevamente en este caso de orden 12. Anteriormente se calculó y representó gráficamente el suavizador de media móvil en la @fig-smap.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(tswge)\n    data(AirPassengers)\n    AirPass.sm12=ma.smooth.wge(AirPassengers,order=12)\n    AirPass.sm12=ts(AirPass.sm12$smooth,start=c(1949,1),frequency=12)\n    ```\n    \n    ::: {.cell-output-display}\n    ![Datos de pasajeros aéreos con suavizado de orden 12.](series_files/figure-html/fig-smap-1.png){#fig-smap width=672}\n    :::\n    :::\n\n\n    Es importante recordar que, en relación con el modelo estimado en @eq-muldecom, $\\hat{tr_{_t}} =$ **AirPass.sm12**. Esta curva casi lineal se muestra como parte de la descomposición completa en la @fig-descmul.\n\nb.  ***Eliminar el Componente de Tendencia de los Datos:*** El siguiente paso consiste en eliminar el componente de tendencia estimado del conjunto de datos de AirPassengers. Esto se puede lograr mediante la división (en lugar de la resta).\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    seas.AirPass=AirPassengers/AirPass.sm12\n    ```\n    :::\n\n\n    La conducta estacional de un año a otro resulta mucho más clara en la @fig-seasmul-1 después de eliminar la \"interferencia\" de la tendencia y el aumento de la variabilidad dentro del año. También se observa que la variabilidad dentro del año no está aumentando tanto como en la @fig-airpass. El incremento en la variabilidad en el modelo final @eq-muldecom se debe a la tendencia creciente que se multiplica por los datos estacionales en la @fig-seasmul-1. Los patrones estacionales en la @fig-seasmul-1 son similares a los de los datos aditivos mostrados en la @fig-seasair-1.\n\n\n    \n\n::: {#fig-seasmul .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![AirPassengers Datos/tendencia](series_files/figure-html/fig-seasmul-1.png){#fig-seasmul-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Componente estacional estimado](series_files/figure-html/fig-seasmul-2.png){#fig-seasmul-2 width=672}\n:::\n\nDatos de AirPassengers sin el componente de tendencia y componente estacional estimado\n:::\n\n\nc.  ***Calcular un \"Promedio\" del Componente Estacional Dentro del Año:*** Al igual que en el modelo aditivo, en el modelo @eq-mulseas , el componente estacional es un patrón general que se supone igual de un año a otro, y el componente de ruido, $z_{_t}$, se ajusta a las variaciones de un año a otro con respecto al patrón estacional general. El componente estacional estimado, $\\hat{s_{_t}}$ (que es idéntico para cada año), se representa en la @fig-seasmul-2. Se observa la similitud entre la @fig-seasmul-2 y la @fig-seasair-2, que fue el componente estacional para la descomposición aditiva de los datos logAirPassengers.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    seas.AirPass.numeric=as.numeric(seas.AirPass) \n    seas.AirPass.matrix=matrix(seas.AirPass.numeric,ncol=12) \n    seas.AirPass.matrix.t=t(seas.AirPass.matrix) \n    months=colMeans(seas.AirPass.matrix.t,na.rm=TRUE) \n    seas.means=rep(months,12) \n    seas.means=ts(seas.means,start=c(1949,1),frequency=12)\n    ```\n    :::\n\n\nd.  ***Encontrar el componente de ruido restante:*** El ruido estimado en @eq-muldecom, $\\hat{z_{_t}}$, es calculado de la siguiente manera\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    Air.Pass.noise = AirPassengers / (AirPass.sm12 * seas.mean)\n    plot(decompose(AirPassengers))\n    ```\n    \n    ::: {.cell-output-display}\n    ![Descomposición multiplicativa de AirPassengers](series_files/figure-html/fig-descmul-1.png){#fig-descmul width=672}\n    :::\n    :::\n\n\nLa @fig-descmul muestra un gráfico de los datos de AirPassengers junto con las partes del procedimiento de descomposición.\n:::\n\n</div>\n:::\n:::\n\n### Ajuste estacional\n\n#### Ajuste estacional aditivo\n\n::: {style=\"text-align: justify\"}\nLos ajustes estacionales están relacionados con las descomposiciones discutidas previamente. Si una descomposición aditiva es apropiada, entonces se utiliza un ajuste estacional aditivo. Un emparejamiento similar se aplica en el caso multiplicativo. El método de ajuste estacional aditivo más directo, consiste en obtener los datos ajustados estacionalmente, $\\hat{sa_{_t}}$, utilizando la fórmula\n\n$$\n\\hat{sa_{_t}}=x_{_t}-\\hat{s_{_t}},\n$$ {#eq-AEadd}\n\nque resta el componente estacional (mostrado en la @fig-seasair-2) de los datos.\n:::\n\n#### Ajuste estacional multiplicativo\n\n::: {style=\"text-align: justify\"}\nDado que la descomposición multiplicativa fue apropiada para los datos de Pasajeros Aéreos, se empleará un ajuste estacional multiplicativo para este conjunto de datos. Similar al método utilizado para el ajuste estacional aditivo, los datos ajustados estacionalmente utilizan la fórmula\n\n$$\n\\hat{sa_{_t}}=x_{_t}/\\hat{s_{_t}},\n$$ {#eq-AEMul}\n\npara dividir los datos por el componente estacional (mostrado en la @fig-seasmul-2).\n\n::: {#exm-aeair}\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n##### AirPassengers\n\nLa Figura @fig-AEmul-1 muestra los datos de Pasajeros Aéreos superpuestos con la estimación de tendencia obtenida utilizando un suavizador de media móvil centrada de orden 12. La @fig-AEmul-2 es una representación gráfica de los datos ajustados estacionalmente calculados utilizando @eq-AEMul. Es decir, es una representación de **AirPassengers.adj**. Una vez más, los datos ajustados estacionalmente son similares a la tendencia estimada pero con más detalle respecto a los cambios mensuales.\n\nEl análisis de la @fig-AEmul-3 muestra que el ajuste estacional utilizando **seas** es más suave y se ve menos afectado por el aumento de la variabilidad dentro del año en años posteriores. En general, los resultados son similares a los vistos en la @fig-AEmul-2 con los efectos estacionales eliminados.\n\n\n::: {#fig-AEmul .cell layout-ncol=\"3\"}\n\n```{.r .cell-code}\nAirPassengers.adj = AirPassengers/seas.means\nlibrary(seasonal) \nlibrary(tswge)\ndata(AirPassengers)\nAirPass.sm12=ma.smooth.wge(AirPassengers,order=12)\nAirPass.sm12=ts(AirPass.sm12$smooth,start=c(1949,1),frequency=12)\ncensus=seas(AirPassengers)\nplot(AirPassengers.adj)\nplot(census$data[,3])\n```\n\n::: {.cell-output-display}\n![AirPassengers con suavizamiento](series_files/figure-html/fig-AEmul-1.png){#fig-AEmul-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Ajuste estacional usando la fórmula](series_files/figure-html/fig-AEmul-2.png){#fig-AEmul-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Ajuste estacional: Census](series_files/figure-html/fig-AEmul-3.png){#fig-AEmul-3 width=672}\n:::\n\nDatos de AirPassengers y ajuste estacional multiplicativo\n:::\n\n:::\n:::\n:::\n\n## Pronóstico y métodos predictivos.\n\n::: {style=\"text-align: justify\"}\nUna aplicación principal del análisis de series temporales es la de realizar pronósticos. Anteriormente se mencionó que uno de los propósitos del suavizado es prever valores futuros. De hecho, si existe evidencia de que los patrones previos en un conjunto de datos continuarán en el futuro, se pueden utilizar diversas técnicas para realizar pronósticos.\n\nPor ejemplo, un propietario de negocio puede desear prever la demanda futura de cierto producto para asegurarse de tener la cantidad apropiada de inventario en stock. Una ciudad que toma decisiones sobre infraestructura podría necesitar prever su población en diez años. Un problema difícil pero de interés para muchos en el sector financiero (y para la mayoría de las personas, en realidad) es predecir las fluctuaciones del mercado de valores y de las acciones individuales para que se puedan tomar decisiones de inversión sólidas. Cada uno de estos ejemplos ilustra la aplicabilidad y necesidad de técnicas de pronóstico. Sin una opción mejor, dichos pronósticos a menudo se hacen de manera algo subjetiva, basados en la memoria pasada de eventos similares, rumores o tal vez mediante cálculos intuitivos pero presumidos que proporcionan conjeturas educadas y estimaciones.\n\nAfortunadamente, las técnicas de análisis de series temporales proporcionan una alternativa matemática basada en si las suposiciones matemáticas subyacentes son apropiadas. Esto resulta en algoritmos que calculan pronósticos junto con límites de predicción correspondientes a un determinado nivel de confianza, análogos al cálculo de la media muestral más o menos un margen de error en el entorno de muestra aleatoria no temporal. El escenario típico es que se desarrolle un algoritmo de pronóstico que luego se utilice para predecir un resultado de interés. El pronóstico comprende estimaciones de parámetros que pueden encontrarse utilizando software en un esfuerzo por lograr una capacidad predictiva óptima.\n\n::: remark\nSe utilizarán los términos predicción y pronóstico de manera sinónima.\n:::\n:::\n\n### Suavizador de media móvil predictivo\n\n::: {style=\"text-align: justify\"}\nEn la @sec-mas se explicó cómo se utiliza el suavizador de media móvil centrada para visualizar una versión suavizada de los datos con el propósito de recuperar señales subyacentes o eliminar ruido o efectos estacionales. También se pueden emplear medias móviles para la predicción. En lugar de la media móvil centrada discutida en la sección mencionada, se utilizará el promedio móvil predictivo para prever valores futuros. Si los datos no exhiben estacionalidad o tendencia, entonces para predecir el valor de la serie temporal en el instante $t+1$, tiene sentido 'predecir' $x_{_{t+1}}$ como el promedio de los $k$ valores de datos anteriores para algún $k$. Es decir, dejando que $\\tilde{x}_{_{t+1}}$ denote la predicción de un paso hacia adelante de $x_{_{t+1}}$ dada la información hasta el tiempo $t$, entonces un predictor razonable y muy simple sería\n\n$$\n\\tilde{x}_{_{t+1}}=\\left(\\sum\\limits_{i=0}^{k+1}x_{_{t-i}}\\right)/k\n$$\n\nEsto es, el predictor de $x_{_{t+1}}$ es el promedio de los últimos $k$ valores de datos.\n:::\n\n::: {.callout-caution collapse=\"true\" style=\"text-align: justify\" icon=\"false\"}\n## Ejemplos\n\n-   Si se quiere usar un predictor de promedio móvil de 3 puntos para un conjunto de datos de longitud $t=20$. Se observa que la predicción un paso hacia adelante de $x_4$ usando este predictor sería\n\n    $$\n    \\tilde{x}_{_4}=(x_{_3}+x_{_2}+x_{_1})/3\n    $$\n\n    Dado que el conjunto de datos tiene 20 valores, hay un valor conocido para $x_{_4}$, por lo que podemos comparar el predictor $\\tilde{x}_{_4}$ con el valor real, $x_{_4}$, para evaluar la precisión del predictor.\n\n-   Para predecir los valores $x_{_4}$ hasta $x_{_n}$, la predicción es\n\n    $$\n    \\tilde{x}_{_n}=(x_{_{n-1}}+x_{_{n-2}}+x_{_{n-3}})/3\n    $$\n\n    Nuevamente, todos estos predictores de un paso hacia adelante pueden compararse con los valores reales para determinar la calidad de cada predicción.\n\n-   La predicción de $x_{_{n+1}}$ está dado por\n\n    $$\n    \\tilde{x}_{_{n+1}}= (x_{_n}+x_{_{n-1}}+x_{_{n-2}})/3\n    $$\n\n    En este caso, $\\tilde{x}_{_{n+1}}$ es un predictor de un valor futuro que presumiblemente no se conoce. Se puede hacer una evaluación sobre la calidad de esta predicción basándose en las predicciones de $x_{_4},x_{_5},\\ldots,$ todas las cuales pueden ser verificadas. El procedimiento se ilustra en el @exm-ppm.\n:::\n\n### Suavizado exponencial\n\n::: {style=\"text-align: justify\"}\nSi bien la predicción mediante el promedio móvil es fácil de conceptualizar y calcular, resulta poco realista asumir que todos los valores de datos precedentes tendrán una influencia igual en los valores futuros. Resulta más intuitivo pensar que, en muchos casos, los datos más recientes deberían tener mayor influencia en los valores futuros que los datos en un pasado más distante, ya que son más representativos del estado actual de la realidad. Un método de suavizado que tiene en cuenta esto se conoce como *suavizado exponencial*. El suavizado exponencial fue introducido por primera vez por R.G. Brown. Nuevamente se considera que cada valor de datos $x_{_t}$ en una serie temporal está compuesto por un valor medio en cada punto de tiempo $t$ y un término de error independiente con media cero y varianza constante. Es decir, $x_{t}=\\mu_{_t}+e_{_t}$. Para $0 \\leq \\alpha \\leq 1$, la recursión de suavizado exponencial para $t =1, 2, \\ldots, n$ es\n\n$$\nu_{_{t+1}}=\\alpha x_{_t}+ (1-\\alpha)u_{_t}\n$$ {#eq-sexpo}\n\nLa @eq-sexpo es una combinación lineal del valor actual $x_{_t}$ junto con $u_{_t}$, que es la estimación de $\\mu_{_t}$ basada en datos hasta, pero no incluyendo, el tiempo $t$. En el momento $t$, el promedio ponderado dado en la expresión anterior es el predictor de $\\mu_{_{t+1}}$. Nótese que la estimación $\\mu_{_t}$ depende en gran medida del parámetro de suavizado $\\alpha$, que varía de $0$ a $1$. Cuanto más cercano esté $\\alpha$ a $1$, más peso se le otorga a los datos más recientes, mientras que cuanto más cercano esté $\\alpha$ a $0$, menos peso se le otorga a los datos más recientes. Naturalmente, el valor óptimo de $\\alpha$ dependerá de la aplicación particular del conjunto de datos y se puede ajustar en consecuencia. Debido a que la fórmula anterior es recursiva, cada nueva estimación $\\mu_{_t}$ depende de la estimación anterior $\\mu_{_{t-1}}$, que a su vez depende de la estimación $\\mu_{_{t-2}}$, y así sucesivamente. Esta fórmula no parece ser \"exponencial\" de inmediato, ¿entonces cómo recibe el método su nombre? Dejando $t = 1, 2, 3, 4,$ vemos que la fórmula recursiva produce lo siguiente:\n\n$$\n\\begin{split}\nu_{_1} &= x_{_1}\\\\\nu_{_2} &= \\alpha x_{_1}+(1-\\alpha)u_{_1}\\\\ &=\\alpha x_{_1}+(1-\\alpha)x_{_1}\\\\ &=x_{_1}\\\\\nu_{_3} &= \\alpha x_{_2}+(1-\\alpha)u_{_2}\\\\ &=\\alpha x_{_2}+(1-\\alpha)x_{_1}\\\\\nu_{_4} &= \\alpha x_{_3}+(1-\\alpha)u_{_3}\\\\ &=\\alpha x_{_3}+(1-\\alpha)[\\alpha x_{_2}+(1-\\alpha)x_{_1}]\\\\ &=\\alpha x_{_3} + \\alpha(1-\\alpha)x_{_2}+(1-\\alpha)^2x_{_1}\\\\\nu_{_5} &= \\alpha x_{_4}+(1-\\alpha)u_{_4}\\\\ &=\\alpha x_{_4}+(1-\\alpha)[\\alpha x_{_3}+\\alpha(1-\\alpha)x_{_2}+(1-\\alpha)^2x_{_1}]\\\\ &=\\alpha x_{_4} + \\alpha(1-\\alpha)x_{_3}+\\alpha(1-\\alpha)^2 x_{_2}+(1-\\alpha)^3x_{_1}\n\\end{split}\n$$\n\nY en general,\n\n$$\nu_{_k}=\\sum_{j=1}^{k-2}\\alpha(1-\\alpha)^{j-1}x_{_{k-j}}+(1-\\alpha)^{k-2}x_{_1}.\n$$ {#eq-fexp}\n\nEs claro a partir de la @eq-fexp que dado que $\\alpha$ está entre cero y uno, la fórmula recursiva otorga menos peso a las observaciones anteriores y más peso a las observaciones más recientes, y que la magnitud del ponderado es exponencial.\n:::\n\n::: {style=\"text-align: justify\"}\n::: {#exm-ppm}\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n## Predicción por promedio móvil y suavizado exponencial de AirPassengers\n\nLa @fig-PAP-1 muestra la gráfica de los datos de AirPassengers. La función ***ma.pred.wge*** implementada en el software R, que hace parte de la librería **tswge**, calcula predicciones a un paso utilizando un suavizador de media móvil de quinto orden. Utilizando los datos, la función extiende las predicciones hasta $x_{_{n+20}}$. La @fig-PAP-2 representa tanto los datos como las predicciones.\n\nLa @fig-PAP-3 muestra los valores $u_{_t}$, exhibe pronósticos a un paso utilizando suavizado exponencial con $\\alpha = 0.4$. Los predictores resultantes son similares a los de la @fig-PAP-2.\n\n\n::: {#fig-PAP .cell layout-ncol=\"3\"}\n\n```{.r .cell-code}\nlibrary(tswge) \nplot(AirPassengers) \n# Predicción por promedio móvil \nma.pred.wge(AirPassengers,order=5,n.ahead=20) \n# Predicción por suavizamiento exponencial \nexpsmooth.wge(AirPassengers,alpha=0.4,n.ahead=20)\n```\n\n::: {.cell-output-display}\n![Datos de AirPassengers](series_files/figure-html/fig-PAP-1.png){#fig-PAP-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Predicción de AirPassengers por promedio móvil](series_files/figure-html/fig-PAP-2.png){#fig-PAP-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Predicción de AirPassengers por suavizamiento exponencial](series_files/figure-html/fig-PAP-3.png){#fig-PAP-3 width=672}\n:::\n\nDatos de AirPassengers, predicción por promedio móvil y suavizado exponencial\n:::\n\n:::\n:::\n:::\n\n### Pronóstico Holt-Winters\n\n::: {style=\"text-align: justify\"}\nEl enfoque Holt-Winters es una técnica desarrollada por los economistas Holt [@holt1957forecasting] y Winters [@winters1960forecasting].\n\nEste método de predicción es una extensión del suavizado exponencial y se aplica a series temporales univariadas. El método no necesita un gran almacenamiento de datos y es simple. Es adecuado para la predicción a corto plazo y utiliza la función de máxima verosimilitud para estimar los parámetros. Existen dos modelos de Holt-Winter que utilizan modelos aditivos o multiplicativos basados en el componente estacional. Los modelos aditivos se aplican para un modelo con una tendencia lineal y con una tendencia exponencial.\n:::\n\n#### Ecuaciones aditivas de Holt-Winters\n\n::: {style=\"text-align: justify\"}\nEl modelo aditivo de Holt-Winters para datos con tendencia y estacionalidad que no aumentan con el tiempo es adecuado (consulte la @eq-addseas). Las fórmulas para la predicción de Holt-Winters son generalizaciones de las ecuaciones de suavizado exponencial, y las ecuaciones de Holt-Winters son las siguientes:\n\n$$\n\\begin{split}\nu_{_t}&= \\alpha(x_{_t}-s_{_{t-m}})+(1-\\alpha)(u_{_{t-1}}+v_{_{t-1}})\\\\\nv_{_t}&= \\beta(u_{_t}-u_{_{t-1}})+(1-\\beta)v_{_{t-1}}\\\\\ns_{_t}&= \\gamma(x_{_t}-u_{_{t-1}})+(1-\\gamma)s_{_{t-m}}\n\\end{split}\n$$ {#eq-extexp}\n\nDonde $0 \\le \\alpha, \\beta, \\gamma \\le 1$, y donde $m$ es la longitud del período. Para datos mensuales, $m = 12$, y para datos trimestrales, $m = 4$. Los $u_{_t}$ están relacionados con el suavizado exponencial simple y proporcionan una línea de base. Los $v_{_t}$ y $s_{_t}$ se relacionan con los efectos de tendencia y estacionales, respectivamente. Para los tiempos $t= m+1,\\ldots, n$, las predicciones de un paso adelante, $\\hat{x}_{_t}$, para la media en el tiempo $t$, se expresan como:\n\n$$ \\hat{x}_{_t}= u_{_{t-1}}+v_{_{t-1}}+s_{_{t-m}} $$\n\nLas predicciones para $x_{_{n+l}}, l=1,\\ldots,K$ (es decir, hasta $K$ pasos más allá del final de los datos observados), se proporcionan de manera recursiva mediante:\n\n$$\n\\hat{x}_{_{n+l|n}}=u_{_n}+mv_{_n}+s_{_{n+l-ml'.}}\n$$\n\nDonde $l' =\\left[ \\frac{l-1}{m} \\right] + 1$, con $\\left[ \\frac{l-1}{m} \\right]$ denotando el entero mayor o igual a $\\frac{l-1}{m}$. Aquí, $\\alpha, \\beta$ y $\\gamma$ son parámetros de suavizado, y se pueden obtener utilizando la función **HoltWinters** la cual parte de las funciones base que conforman al software *R*.\n:::\n\n#### Ecuaciones multiplicativas de Holt-Winters\n\n::: {style=\"text-align: justify\"}\nComo sugiere el término, las ecuaciones multiplicativas de Holt-Winters son aplicables a datos para los cuales el modelo multiplicativo la @eq-mulseas es apropiado. En este caso, las ecuaciones de Holt-Winters son:\n\n$$\n\\begin{split}\nu_{_t}&= \\alpha(x_{_t}/s_{_{t-m}})+(1-\\alpha)(u_{_{t-1}}+v_{_{t-1}})\\\\\nv_{_t}&= \\beta(u_{_t}-u_{_{t-1}})+(1-\\beta)v_{_{t-1}}\\\\\ns_{_t}&= \\gamma(x_{_t}/u_{_{t-1}})+(1-\\gamma)s_{_{t-m}}\n\\end{split}\n$$\n\nDonde $0 \\le \\alpha, \\beta, \\gamma \\le 1$, y donde nuevamente, $m$ es la frecuencia. Las predicciones para los valores de $x_{_t}$ se expresan mediante:\n\n$$ \\hat{x}_{_t}= (u_{_{t-1}}+v_{_{t-1}})s_{_{t-m}} $$\n\nLas predicciones para $x_{_{n+l}}, l=1,\\ldots,K$ (es decir, hasta $K$ pasos más allá del final de los datos observados), se proporcionan de manera recursiva mediante:\n\n$$\n\\hat{x}_{_{n+l|n}}=(u_{_n}+lv_{_n})s_{_{n+l-ml'.}}\n$$\n\nDonde $l' =\\left[ \\frac{l-1}{m} \\right] + 1$.\n:::\n\n::: {#exm-HWAP style=\"text-align: justify\"}\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n#### AirPassengers\n\nReconsidere los datos de AirPassengers. Dado que los puntos temporales posteriores revelan magnitudes crecientes de los picos y valles cíclicos, la serie temporal es multiplicativa en lugar de aditiva. La @fig-HWAP-1 muestra las predicciones de un paso adelante de Holt-Winters para los años 1950-1960 (superpuestas a los datos reales) dadas por la fórmula utilizando los parámetros de suavizado y coeficientes estimados anteriormente. Las predicciones de un paso adelante (línea sólida) son bastante precisas y apenas se distinguen de los datos (puntos en la línea). La @fig-HWAP-2 muestra los datos de AirPassengers junto con las predicciones de Holt-Winters para los próximos tres años (línea punteada). Estas predicciones parecen extender con precisión el patrón precedente.\n\n\n::: {#fig-HWAP .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Figure 2.20(a)\nap.hw=HoltWinters(AirPassengers,seasonal=\"mult\")\nplot(ap.hw)\n# Figure 2.20(b)\nap.pred=predict(ap.hw,n.ahead=36)\nplot(ap.hw,ap.pred,lty=1:2)\n```\n\n::: {.cell-output-display}\n![Predicción de AirPassengers un paso adelante](series_files/figure-html/fig-HWAP-1.png){#fig-HWAP-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Predicción por Holt-Winters de AirPassengers](series_files/figure-html/fig-HWAP-2.png){#fig-HWAP-2 width=672}\n:::\n\nPredicción mediante Holt-Winters de los datos de AirPassengers\n:::\n\n:::\n:::\n\n### Modelo Autoregresivo (AR)\n\n::: {#def-mARp style=\"text-align: justify\"}\nSe afirma que el proceso $X_{_t}$ satisface un modelo $\\mathrm{AR}(p)$ (Autoregresivo de orden $p$) si\n\n$$\nX_{_t}=a_{_t}+\\beta+\\sum_{k=1}^p \\phi_{_k}X_{_{t-k}}\n$$ {#eq-ARp}\n\ndonde $\\phi_{_k}, k=1,\\ldots,p$ son constantes reales, $\\beta = \\left(1-\\phi_{_1}-\\phi_{_2}-\\cdots-\\phi_{_p}\\right)\\mu$, $\\phi_{_p}\\neq 0$, y $a_{_t}$ es un proceso de ruido blanco con media cero y varianza finita $\\sigma_{_a}^2$.\n:::\n\n::: {style=\"text-align: justify\"}\nLa fórmula en la @eq-ARp indica que el valor del proceso en el tiempo $t$ es una combinación lineal de los $p$ valores anteriores más un componente de ruido aleatorio en $a_{_t}$. Iniciamos nuestra discusión sobre los modelos $\\mathrm{AR}$ al abordar sus propiedades, incluidas las condiciones de estacionariedad y el comportamiento de las autocorrelaciones y densidades espectrales para modelos específicos.\n\nEl modelo $\\mathrm{AR}(p)$ general definido en la @def-mARp, se asemeja a una ecuación de regresión múltiple, donde, en este caso, las \"variables independientes\" son los $p$ valores previos de la \"variable dependiente\" $X_{_t}$. Otra forma de escribir la @eq-ARp, después de reorganizar los términos, es:\n\n$$\nX_{_t}-\\mu-\\phi_{_1}(X_{_{t-1}}-\\mu)-\\phi_{_2}(X_{_{t-2}}-\\mu)-\\cdots-\\phi_{_p}(X_{_{t-p}}-\\mu)=a_{_t}\n$$ {#eq-oarp}\n\nAl igual que en el caso de los modelos $\\mathrm{AR}(1)$ y $\\mathrm{AR}(2)$, se expresará con frecuencia el $\\mathrm{AR}(p)$ en la forma de media cero:\n\n$$\nX_{_t} -\\phi_{_1}X_{_{t-1}}-\\phi_{_2}X_{_{t-2}}-\\cdots-\\phi_{_p}X_{_{t-p}}=a_{_t}\n$$ {#eq-m0arp}\n\nLas ecuaciones @eq-ARp a @eq-m0arp dan la impresión de que un modelo $\\mathrm{AR}(p)$ será mucho más complicado de manejar que un modelo $\\mathrm{AR}(1)$ o $\\mathrm{AR}(2)$. La comprensión de las características de los modelos $\\mathrm{AR}(1)$ y $\\mathrm{AR}(2)$ conduce directamente a comprender el comportamiento de un modelo $\\mathrm{AR}(p)$.\n:::\n\n##### Hechos sobre el modelo $\\mathrm{AR}(p)$\n\n::: {style=\"text-align: justify\"}\ni.  $\\mathrm{E}[X_{_t}]=\\mu$, para la forma \"no nula de la media\" del modelo $\\mathrm{AR}(p)$ en la @eq-oarp y @eq-m0arp.\n\nii. El proceso de varianza es $$\\sigma_{_X}^2=\\gamma_{_0}=\\frac{\\sigma_{_a}^2}{1-\\phi_{_1}\\rho_{_1}-\\phi_{_2}\\rho_{_2}-\\cdots-\\phi_{_p}\\rho_{_p}}$$\n\n    la cuál es contante y finita cuando $X_{_t}$ es estacionaria.\n\niii. La autocorrelación de un proceso $\\mathrm{AR}(p)$ satisface $$\n     \\rho_{_k}=\\phi_{_p}+\\sum_{n=1}^{p-1}\\phi_{_n}\\rho_{_{k-n}}\n     $$\n\n     La ecuación (5.28) es una generalización de (5.17) para el caso AR(2), y conduce a las ecuaciones de Yule-Walker de orden p p: $$\n         \\begin{split}\n     \\rho_{_{1}} &= \\phi_{_1}+\\phi_{_{2}} \\rho_{_1} +\\ldots + \\phi_{_p} \\rho_{_{p-1}} \\\\\n     \\rho_{_{2}} &= \\phi_{_1}\\rho_{_1}+\\phi_{_{2}}  +\\ldots + \\phi_{_p} \\rho_{_{p-2}}\\\\\n     &\\vdots \\\\\n     \\rho_{_{p}} &= \\phi_{_1}\\rho_{_{p-1}}+\\phi_{_{2}} \\rho_{_{p-2}} +\\ldots + \\phi_{_p}.\n     \\end{split}\n       $$\n\n     Análogo al caso $\\mathrm{AR}(2)$, conocer los valores de $\\phi_{_1}, \\phi_{_2}, \\ldots, \\phi_{_p}$ nos permite resolver este sistema de ecuaciones de dimensión $p\\times p$ para $\\rho_{_k}$, donde $k = 1, 2, \\ldots, p$. Las autocorrelaciones basadas en el modelo, $\\rho_{_k}$, para $k > p$, se pueden calcular utilizando la recursión $\\phi_{_1} \\rho_{_{k-1}} - \\phi_{_2} \\rho_{_{k-2}} + \\ldots + \\phi_{_p} \\rho_{_{k-p}}$. No sorprendentemente, se utilizan funciones computacionales para realizar estos cálculos.\n\niv. La densidad espectral de un modelo $\\mathrm{AR}(p)$ es dada por\n\n    $$\n    S_{_X}(f)=\\frac{\\sigma_{_a}^2}{\\gamma_{_0}|1-\\phi_{_1}e^{-2\\pi i f}-\\phi_{_2}e^{-4\\pi i f}-\\cdots-\\phi_{_p}e^{-2p\\pi i f}|^2}\n    $$\n:::\n\n::: {#def-pac style=\"text-align: justify\"}\n##### Autocorrelaciones parciales\n\nSea $X_{_t}$ un proceso estacionario con autocorrelaciones $\\rho_{_j}=j=0,1,\\ldots$.\n\na.  La autocorrelación parcial en rezago $k$, denotada como $\\phi_{_{kk}}$ , es la correlación entre $X_{_t}$ y $X_{_{t+ k}}$ condicional al \"conocimiento\" de las variables intervinientes $X_{_{t+1}}, X_{_{t+2}}$, y $X_{_{t+k-1}}$.\n\nb.  Considere las siguientes ecuaciones de Yule-Walker donde $\\phi_{_{kj}}$ denota el coeficiente $j-$ésimo asociado con las ecuaciones de Yule-Walker de orden $k$.\n\n    $$\\begin{split} k&=1\\\\ \\rho_{_1}&=\\phi_{_{11}}\\\\ \\\\ k&=2\\\\ \\rho_{_1}&=\\phi_{_{21}}+\\phi_{_{22}}\\rho_{_1}\\\\ \\rho_{_2}&=\\phi_{_{21}}\\rho_{_1}+\\phi_{_{22}}\\end{split}$$\n\n    En general...\n\n    $$\\begin{split}\\rho_{_1}&=\\phi_{_{k1}}+\\phi_{_{k2}}\\rho_{_1}+\\cdots+\\phi_{_{kk}}\\rho_{_{k-1}}\\\\\\rho_{_2}&=\\phi_{_{k1}}\\rho_{_1}+\\phi_{_{k2}}+\\cdots+\\phi_{_{kk}}\\rho_{_{k-2}}\\\\ \\vdots\\\\ \\rho_{_k}&=\\phi_{_{k1}}\\rho_{_{k-1}}+\\phi_{_{k2}}\\rho_{_{k-2}}+\\cdots+\\phi_{_{kk}}\\end{split}$$\n\n    La función de autocorrelación parcial se define como $\\phi_{_{kk}}, k = 1, 2,...$.\n:::\n\n##### Notación de operador y ecuación característica para un $\\mathrm{AR}(p)$\n\n::: {style=\"text-align: justify\"}\nEl modelo $\\mathrm{AR}(p)$ en la @eq-oarp puede ser escrito en notación de operador como\n\n$$\n(1-\\phi_{_1}B-\\phi_{_2}B^2-\\cdots-\\phi_{_p}B^p)(X_{_t}-\\mu)=a_{_t}\n$$\n\nO utilizando una notación abreviada, $\\phi(B)(X_{t}-\\mu)=a_{_t}$ , donde $\\phi(B)$ es el operador de orden $p$\n\n$$\n\\phi(B)=1-\\phi_{_1}B-\\phi_{_2}B^2-\\cdots-\\phi_{_p}B^p.\n$$\n\nConvirtiendo el operador $\\phi(B)$ en la cantidad algebraica $\\phi(z)$ resulta en el polinomio característico general $\\mathrm{AR}(p)$\n\n$$\n\\phi(z)=1-\\phi_{_1}z-\\phi_{_2}z^2-\\cdots-\\phi_{_p}z^p\n$$\n\nLa correspondiente ecuación característica $\\mathrm{AR}(p)$ es\n\n$$\\phi(z) = 1 -\\phi_{_1}z -\\phi_{_2}z^2 - \\cdots - -\\phi_{_p}z^p = 0$$\n\nLa ecuación característica tiene $p$ raíces $r_{_1}, r_{_2} ,\\ldots, r_{_p}$ que son reales y/o complejas, donde las raíces complejas aparecen como pares conjugados y algunas raíces pueden ser repetidas.\n:::\n\n::: {#thm-arpest style=\"text-align:justify\"}\nUn proceso $\\mathrm{AR}(p)$ es estacionario si y solo si todas las raíces de la ecuación característica son mayores que uno en valor absoluto.\n\n::: proof\nVea @harvey1981econometric.\n:::\n:::\n\n::: {#exm-ar4}\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n###### Un modelo $\\mathrm{AR}(4)$\n\nConsidere el modelo $\\mathrm{AR}(4)$\n\n$$\nX_{_t}-0.13X_{_{t-1}}-1.4414X_{_{t-2}}+.0326X_{_{t-3}}+.8865X_{_{t-4}}=a_{_{t}}\n$$ {#eq-ejar4}\n\ndonde $\\sigma_{_a}^2 = 1$. La notación del operador para este modelo es\n\n$$ (1 - 0.13B + 1.4414B^2 - .0326B^3 + 0.8865 B^4)X_{_t}= a_{_t},$$\n\ny la correspondiente ecuación característica es\n\n$$1 - 0.13z + 1.4414z^2 - .0326z^3 + 0.8865 z^4 = 0.$$\n\nLa @fig-ejar4 representa una realización de longitud $n = 200$ del proceso descrito en la @eq-ejar4, junto con las autocorrelaciones muestrales asociadas y la estimación de la densidad espectral Parzen, respectivamente.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tswge)\nx=gen.arma.wge(n=200,phi=c(0.1300,1.4414,-.0326,-.8865),sn=9310,plot=FALSE)\nplotts.sample.wge(x)\n```\n\n::: {.cell-output-display}\n![Realización del modelo AR(4)](series_files/figure-html/fig-ejar4-1.png){#fig-ejar4 width=672}\n:::\n:::\n\n:::\n:::\n\n##### El Test Aumentado de Dickey-Fuller\n\n::: {style=\"text-align: justify\"}\nEste test ha estado en uso durante muchos años para probar las hipótesis:\n\n-   $H_{_0}:$ el modelo contiene una raíz unitaria\n\n-   $H_{_a}$: el modelo no contiene una raíz unitaria (y por lo tanto es estacionario)\n\nEstadístico de prueba: $\\uptau$\n\nRegión de rechazo: Rechazar $H_{_0}$ si $\\uptau < d_{_\\alpha}$ donde $d_{_\\alpha}$ es el valor crítico de nivel $\\alpha$.\n\n@dickey1976estimation obtiene la distribución límite (complicada) del estadístico de prueba. Si $\\uptau \\geq d_{_{.05}}$, entonces no se rechaza $H_{_0}$ y la prueba de Dickey-Fuller detecta una raíz unitaria. Nótese que el rechazo de la hipótesis nula lleva a la conclusión de que el proceso es estacionario. Por lo tanto, la conclusión de una raíz unitaria se basa en no rechazar la hipótesis nula. Es importante recalcar que no rechazar la hipótesis nula no implica creer que la hipótesis nula sea verdadera, sino simplemente que no hubo suficiente evidencia para rechazarla.\n\nPara llevar a cabo pruebas de raíz unitaria, se empleará una implementación del test del software estadístico R. Existen diversas opciones, y se utilizará el siguiente comando que incorpora una constante pero no una tendencia en el modelo, y utiliza el criterio de información de Akaike (AIC) para seleccionar el número de rezagos. Se recomienda consultar las obras de @dickey1979distribution o @fuller1995introduction para obtener más detalles al respecto.\n:::\n\n##### Factorización del polinomio característico de un $\\mathrm{AR}(p)$\n\n::: {style=\"text-align: justify\"}\nLas raíces de una ecuación cuadrática se pueden encontrar mediante el uso de la fórmula cuadrática. Sin embargo, las cosas se vuelven más complicadas para órdenes polinómicos mayores a dos. La ecuación cúbica\n\n$$\n1-2.1z+1.6z^2-.3z^3=0\n$$\n\npuede factorizarse en la forma $(1-.5z)(1-1.6z+.8z^2)$. Basándose en esta factorización, las raíces que se obtienen son $r_{_1} = 1/.5=2$, $r_{_2} = 1+.5i$ y $r_{_3} = 1-.5i$. Es decir, este $\\mathrm{AR}(3)$ tendrá un comportamiento de primer orden asociado con $1-.5B$ (es decir, una frecuencia de cero), un comportamiento cíclico de segundo orden con una frecuencia de sistema $f_{_0} = 0.07$, y el proceso es estacionario porque todas las raíces están fuera del círculo unitario.\n:::\n\n::: {#thm-poli style=\"text-align: justify\"}\nEl polinomio de orden $p$, $1-\\phi_{_1}z-\\phi_{_2}z^2-\\cdots-\\phi_{_p}z^p$, siempre puede descomponerse como un producto de\n\ni.  factores de primer orden (lineales) asociados con raíces reales\n\nii. factores de segundo orden (cuadráticos) cuyas raíces son pares conjugados complejos.\n:::\n\n##### Tablas de factores para modelos $\\mathrm{AR}(p)$\n\n::: {style=\"text-align: justify\"}\nEl @thm-poli establece que cualquier polinomio de orden $p$ puede expresarse como un producto de factores de primer orden y/o factores irreducibles de segundo orden. Comprender los factores de primer y segundo orden es la clave para comprender el modelo $\\mathrm{AR}(p)$.\n\n::: {.callout-caution collapse=\"true\" icon=\"false\"}\n###### @exm-ar4 (Continuación...)\n\nSe considera de nuevo el modelo $\\mathrm{AR}(4)$ en @eq-ejar4. La ecuación característica asociada es\n\n$$\n1-.13z-1.4414z^2+.0326z^3+.8865z^4=0\n$$\n\nLa forma factorizada (obtenida numéricamente) es\n\n$$\n(1-1.89B+.985B^2)(1+1.76B+.9B^2)=0\n$$\n\nLa tabla de factores es una herramienta muy útil para resumir rápidamente la escencia de un modelo $\\mathrm{AR}(p)$ con respecto a los factores de primer y segundo orden.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tswge)\nfactor.wge(phi=c(.13,1.4414,-.0326,-.8865))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  \n  \nCoefficients of AR polynomial:  \n0.1300 1.4414 -0.0326 -0.8865 \n\n                           AR Factor Table \nFactor                 Roots                Abs Recip    System Freq \n1-1.8900B+0.9850B^2    0.9594+-0.3079i      0.9925       0.0494\n1+1.7600B+0.9000B^2   -0.9778+-0.3938i      0.9487       0.4391\n  \n  \n```\n\n\n:::\n:::\n\n\nEn este caso, hay dos factores de segundo orden, $1-1.89B+.985B^2$ y $1+1.76B+.9B^2$.\n:::\n:::\n\n## Evaluación de la precisión de los pronósticos\n\n::: {style=\"text-align: justify\"}\nPara obtener una cuantificación \"general\" de la calidad de las predicciones, se evalúa qué tan bien coinciden las predicciones ($f_{_t}$) con los valores reales ($y_{_t}$) en el tiempo $t$. Afortunadamente, se han ideado métricas de error para evaluar la calidad del modelo y permitir la comparación con otras regresiones que poseen diferentes parámetros. Estas métricas son resúmenes breves pero informativos de la calidad de los datos. A continuación se presentan algunas métricas de rendimiento más comunes (@segall2022biomedical).\n:::\n\n::: {style=\"text-align: justify\"}\n### MAE\n\nEl error absoluto medio (MAE) se calcula tomando el residuo para cada punto de datos, considerando únicamente el valor absoluto para minimizar el impacto de los valores atípicos en comparación con @eq-RMSE. Luego, se obtiene el promedio de todos estos residuos. La ecuación formal se presenta a continuación: $$MAE= \\frac{1}{n}\\sum_{t=1}^n |y_{_t}-f_{_t}|$$ {#eq-MAE} Dado que se emplea el valor absoluto del residuo, no se indica el rendimiento inferior o superior del modelo. Cada residuo contribuye de manera equitativa al error total, y los errores más grandes tienen una mayor contribución al error general. Un MAE pequeño indica un buen rendimiento de predicción, mientras que un MAE grande sugiere que el modelo puede tener dificultades en ciertas áreas. Obtener MAE perfecta de 0 es rara, indica que el modelo es un predictor impecable.\n\nSin embargo, el uso del valor absoluto del residuo puede no ser el mejor enfoque para interpretar los datos, ya que los *valores atípicos* (es decir, los puntos de datos que se alejan significativamente de la tendencia general de los datos) pueden afectar significativamente el rendimiento del modelo. Dependiendo del tratamiento de los valores atípicos y extremos en los datos, es posible que se desee resaltar o minimizar su impacto. Como resultado, la elección de la métrica de error adecuada puede verse influida por el problema de los valores atípicos.\n:::\n\n::: {style=\"text-align: justify\"}\n### MSE\n\nEl error cuadrático medio (MSE) es similar al MAE, pero eleva al cuadrado la diferencia antes de sumarlos todos en lugar de utilizar el valor absoluto. Esta diferencia se puede observar en la siguiente ecuación: $$MSE=\\frac{1}{n} \\sum_{t=1}^n (y_{_t}-f_{_t})^2$$ {#eq-MSE} El error medio absoluto (MAE) y el error cuadrático medio (MSE) son métricas de error comúnmente utilizadas en la evaluación de modelos. Sin embargo, el MSE suele ser mayor que el MAE debido al cuadrado de la diferencia. Comparar los dos directamente no siempre es posible, y en su lugar, debemos comparar las métricas de error de nuestro modelo con las de un modelo ya conocido o que se ajuste a la serie de datos. El efecto de los valores atípicos en nuestros datos es más evidente con la presencia del término cuadrado en la ecuación MSE. Mientras que cada residuo en MAE contribuye proporcionalmente al error total, el error crece cuadráticamente en MSE. En última instancia, esto significa que los valores atípicos en nuestros datos contribuirán a un error total mucho mayor en el MSE que en el MAE. Del mismo modo, nuestro modelo se verá más penalizado por hacer predicciones que difieran mucho del valor real correspondiente.\n:::\n\n::: {style=\"text-align: justify\"}\n### RMSE\n\nRMSE, o error cuadrático medio, es una medida frecuentemente utilizada para evaluar la diferencia entre los valores predichos $f_{_t}$ y los valores observados $y_{_t}$. Su función se expresa a continuación, donde $n$ representa el número de observaciones. En comparación con el error cuadrático medio (MSE), RMSE toma la raíz cuadrada de MSE y restituye la unidad al mismo nivel que la variable dependiente. Por lo tanto, tiene la ventaja de ser interpretado directamente. En general, un valor de RMSE más bajo es preferible, y RMSE$=0$ indica un ajuste perfecto de los datos. La desventaja de RMSE es su sensibilidad a valores atípicos, ya que unos pocos errores grandes en la suma pueden generar un aumento significativo, y la prueba no distingue entre subestimación y sobreestimación. Como se discutió anteriormente en la descripción de los datos, el conjunto de datos que se utiliza tiene varios valores extremos de gastos elevados, por lo que utilizar solo RMSE como medida podría no ser muy adecuado.\n\nLa fórmula para el cálculo de RMSE es la siguiente: $$RMSE= \\sqrt{\\frac{\\sum_{t=1}^n (y_t-f_t)^2}{n}}$$ {#eq-RMSE}\n:::\n\n::: {style=\"text-align: justify\"}\n### MAPE\n\nEl error porcentual absoluto medio (MAPE) mide la precisión de la predicción como un porcentaje y se define generalmente de la siguiente manera. La ventaja es que es muy intuitivo interpretar el error relativo, y un MAPE más bajo significa un error menor. MAPE es similar a MAE, pero normaliza MAE mediante observaciones reales, resolviendo así el problema de que MAE proporciona poca información sobre el error al comparar datos de diferentes escalas. Sin embargo, también presenta la desventaja de que puede producir valores infinitos o indefinidos para valores reales cercanos o iguales a cero. Otra limitación de MAPE es que penaliza más los errores negativos que los errores positivos. Por ejemplo, para un valor real de $100$ y un valor estimado de $90$, el MAPE es $0.10$. Para el mismo valor estimado y un valor real de $80$, el MAPE es $0.125$. Como resultado, si se utiliza MAPE como función objetivo, el estimador preferirá valores más pequeños y puede sesgarse hacia errores negativos. $$MAPE=\\frac{100}{n}\\sum_{t=1}^n \\left| \\frac{y_t-f_t}{y_t}\\right|$$ {#eq-MAPE}\n:::\n",
    "supporting": [
      "series_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}