{
  "hash": "7afa1b94dc36ab3b483faf363a02c47c",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlang: es\n---\n\n\n\n\n\n# Estadística\n\n::: {style=\"text-align: justify\"}\nLa estadística tiene un origen que se remonta a tiempos antiguos, cuando las civilizaciones antiguas recopilaban y analizaban datos para tomar decisiones informadas en diversas áreas. Sin embargo, fue en el siglo XVII cuando los trabajos de pensadores como John Graunt y William Petty sentaron las bases para los métodos estadísticos modernos. Graunt realizó estudios sobre la mortalidad y estableció principios de recopilación de datos, mientras que Petty aplicó el análisis estadístico a la economía y la demografía.\n\nEn la era moderna, con el advenimiento de la computación y la disponibilidad de grandes volúmenes de datos, la estadística ha cobrado una importancia aún mayor. Técnicas avanzadas como el análisis de series de tiempo, la regresión múltiple, el análisis de componentes principales y el aprendizaje automático han transformado la forma en que se aborda la predicción de datos. Estas herramientas permiten modelar relaciones complejas y patrones ocultos en los datos, lo que es crucial para la toma de decisiones en áreas como el marketing, la medicina, la economía y la planificación empresarial.\n\n[@mann2010introductory] presenta dos significados para la palabra \"estadística\". En el sentido más común, la estadística hace referencia a hechos numéricos. El segundo significado de estadística se relaciona con el campo o disciplina de estudio. Bajo esta perspectiva, la estadística se define de la siguiente manera\n:::\n\n::: {#def-estad style=\"text-align: justify\"}\n## Estadística\n\nUna estadística es una función de variables aleatorias observables, la cual es en sí misma una variable aleatoria observable y no contiene ningún parámetro desconocido.\n:::\n\n## Datos univariados\n\n::: {style=\"text-align: justify\"}\nEn esta sección se definirán conceptos básicos de la estadística univariada. Se comienza con los siguientes conceptos;\n:::\n\n::: {#def-pobla style=\"text-align: justify\"}\n### Población\n\nUna *población* consiste en todos los elementos (individuos, elementos u objetos) cuyas características se están estudiando. La población que se está estudiando también se denomina *población objetivo*.\n:::\n\n::: {#def-param style=\"text-align: justify\"}\n### Parámetro\n\nUn *parámetro* es una característica numérica o descriptiva de una población o probabilidad distribución.\n:::\n\n### Valor Esperado y momentos\n\n::: {style=\"text-align: justify\"}\nUn concepto sumamente útil en problemas que implican variables aleatorias o distribuciones es el de la esperanza (valor esperado). En esta subsección, se presentan definiciones y resultados relacionados con la esperanza.\n:::\n\n::: {#def-mean style=\"text-align: justify\"}\n#### Media\n\nSea $X$ una variable aleatoria. La *media* de $X$, denotada por $\\mu_X$ o $\\mathrm E[X]$, se define como\n\n$$ \\mathrm E[X]= \\sum x_jf_X(x_j) .$$\n\nSi $X$ es discreta con puntos de densidad $x_1, x_2, \\ldots, x_j, \\ldots$\n\n$$ \\mathrm E[X]=\\int_{-\\infty}^\\infty x f_X(x)dx.$$\n\nSi $X$ es continua con una función de densidad de probabilidad $f_X(x)$\n\n$$ \\mathrm E[X]=\\int_0^\\infty [1-F_X(x)]dx-\\int_{-\\infty}^0 F_X(x) dx.$$\n\npara cualquier variable aleatoria $X$.\n:::\n\n::: {.remark style=\"text-align: justify\"}\n$\\mathrm E[X]$ representa el centro de gravedad (o centroide) de la región unitaria determinada por la función de densidad de $X$. De esta manera, la media de $X$ proporciona una medida de la ubicación central de los valores de la variable aleatoria $X$.\n:::\n\n::: {style=\"text-align: justify\"}\nLa media de una variable aleatoria $X$ es una medida de ubicación central de la densidad de $X$. La varianza de una variable aleatoria $X$ es una medida de la dispersión o propagación de la densidad de $X$.\n:::\n\n::: {#def-var style=\"text-align: justify\"}\n#### Varianza\n\nSea $X$ una variable aleatoria. Se define $\\mu_X$ como $\\mathrm E[X]$. La *varianza* de $X$, denotada como $\\sigma_X^2$ o $\\mathrm{Var}[X]$, se define de la siguiente manera\n\n$$ \\mathrm{Var}[X]= \\sum_j (x_j-\\mu_X)^2 f_X(x_j).$$\n\nAhora bien, si $X$ es discreta con puntos $x_1, x_2, \\ldots, x_j, \\ldots$.\n\n$$ \\mathrm{Var}[X]=\\int_{-\\infty}^\\infty (x-\\mu_X)^2f_X(x)dx.$$\n\nSi $X$ es continua con una función de densidad de probabilidad $f_X(x)$\n\n$$ \\mathrm{Var}[X]=\\int_0^\\infty 2x[1-F_X(x)+F_X(-x)]dx - \\mu_X^2 $$\n\npara una variable aleatoria $X$ arbitraria.\n:::\n\n::: {style=\"text-align: justify\"}\nSe vio que una media era el centro de gravedad de una densidad; de manera similar, la varianza representa el momento de inercia de la misma densidad con respecto a un eje perpendicular que pasa por el centro de gravedad.\n:::\n\n::: {#def-SD style=\"text-align: justify\"}\n#### Desviación estándar\n\nSi $X$ es una variable aleatoria, la *desviación estándar* de $X$, denotada por $\\sigma_X$, se define como $+\\sqrt{\\mathrm{Var}[X]}$.\n:::\n\n::: {style=\"text-align: justify\"}\nLa desviación estándar de una variable aleatoria, al igual que la varianza, es una medida de la dispersión o propagación de los valores de la variable aleatoria. En muchas aplicaciones, es preferible a la varianza como medida, ya que tendrá las mismas unidades de medida que la propia variable aleatoria.\n:::\n\n#### Valor esperado de una función de una variable aleatoria\n\n::: {style=\"text-align: justify\"}\nSe definió la esperanza de una variable aleatoria arbitraria $X$, llamada la media de $X$. En esta subsección, se definirá la esperanza de una función de una variable aleatoria para variables aleatorias discretas o continuas.\n:::\n\n::: {#def-VE style=\"text-align: justify\"}\n##### Valor esperado\n\nSea $X$ una variable aleatoria y $g(\\cdot)$ una función con dominio y codominio en la recta real. La esperanza o valor esperado de la función $g(\\cdot)$ de la variable aleatoria $X$, denotada por $\\mathrm E[g(X)]$, se define de la siguiente manera:\n\n$$ \\mathrm E[g(X)]=\\sum_j g(x_j)f_X(x_j). $$ {#eq-VE1}\n\nSi $X$ es discreta con puntos $x_1, x_2, \\ldots, x_j, \\ldots$ (siempre que esta serie sea absolutamente convergente),\n\n$$ \\mathrm E[g(X)]=\\int_{-\\infty}^\\infty g(x)f_X(x)dx.$$ {#eq-VE2}\n\nSi $X$ es continua con función de densidad de probabilidad $f_X(x)$ (siempre que $\\int_{-\\infty}^\\infty |g(x)|f_X(x)dx < \\infty$).\n:::\n\n::: {.remark style=\"text-align: justify\"}\nSi $g(x)=x$, entonces $\\mathrm E[g(X)]=\\mathrm E[X]$ es la media de $X$. Si $g(x)=(x-\\mu_X)^2$, entonces $\\mathrm E[g(X)]=\\mathrm E[(X-\\mu_X)^2]=\\mathrm{Var}[X]$.\n:::\n\n::: {#thm-propve style=\"text-align: justify\"}\nA continuación se presentan las propiedades del valor esperado,\n\ni.  $\\mathrm E[c]=c$ para una constante $c$.\n\nii. $\\mathrm E[cg(X)]=c\\mathrm E[g(X)]$ para una constante $c$.\n\niii. $\\mathrm E[c_1 g_1(X)+c_2 g_2(X)]=c_1\\mathrm E[g_1(X)]+c_2\\mathrm E[g_2(X)]$.\n\niv. $\\mathrm E[g_1(X)]\\leq \\mathrm E[g_2(X)]$ si $g_1(x)\\leq g_2(x)$ para todo $x$.\n:::\n\n::: {#thm-var style=\"text-align: justify\"}\nSi $X$ es una variable aleatoria, entonces\n\n$$\\mathrm{Var}[X] = \\mathrm E[(X-\\mathrm E[X])^2] = \\mathrm E[X^2] - (\\mathrm E[X])^2,$$ {#eq-var} siempre que $\\mathrm E[X^2]$ exista.\n:::\n\n::: {style=\"text-align: justify\"}\nLas pruebas de los teoremas anteriores se pueden consultar en [@mood1986introduction].\n:::\n\n#### Momentos\n\n::: {style=\"text-align: justify\"}\nLos momentos de una variable aleatoria o de una distribución son los valores esperados de las potencias de la variable aleatoria que tiene la distribución dada.\n:::\n\n::: {#def-moment style=\"text-align: justify\"}\n##### Momento\n\nSi $X$ es una variable aleatoria, el $r-$ésimo momento de $X$, generalmente denotado por $\\mu_r'$, se define como\n\n$$ \\mu_r'=\\mathrm E[X^r] $$ si el valor esperado existe.\n:::\n\n::: {style=\"text-align: justify\"}\nNote que $\\mu_1' = \\mathrm E[X] = \\mu_X$, es la media de $X$.\n:::\n\n::: {#def-quant style=\"text-align: justify\"}\n##### Cuantil\n\nEl $q-$ésimo cuantil de una variable aleatoria $X$ o de su distribución correspondiente se denota como $\\xi_q$ y se define como el número más pequeño $\\xi$ que cumple con la condición $F_X(\\xi) \\geq q$.\n:::\n\n::: {style=\"text-align: justify\"}\nSi $X$ es una variable aleatoria continua, entonces el $q-$ésimo cuantil de $X$ se calcula como el número más pequeño $\\xi$ que cumple con la condición $F_X(\\xi) = q$.\n:::\n\n::: {#def-median style=\"text-align: justify\"}\n##### Mediana\n\nLa mediana de una variable aleatoria $X$, denotada como $\\mathrm{med}_X$*,* $\\mathrm{med}(x)$ *o* $\\xi_{0.5}$, es el cuantil $0.5$.\n:::\n\n### Muestreo\n\n::: {#def-muestra style=\"text-align: justify\"}\n#### Muestra\n\nUna porción de la población seleccionada para el estudio es conocida como una *muestra*.\n:::\n\n::: {style=\"text-align: justify\"}\nUna muestra puede ser aleatoria o no aleatoria. En una muestra aleatoria, cada elemento de la población tiene la posibilidad de ser incluido en la muestra. Sin embargo, en una muestra no aleatoria este puede no ser el caso.\n:::\n\n::: {#def-mas style=\"text-align: justify\"}\n#### Muestra aleatoria\n\nSean $X_1, X_2, \\ldots, X_n$ variables aleatorias con una densidad conjunta $f_{(X_1,\\ldots, X_n)}(\\cdot,\\ldots, \\cdot)$ que se descompone de la siguiente manera:\n\n$$f_{X_1,X_2,\\ldots, X_n}(x_1,x_2,\\ldots,x_n)=f(x_1)f(x_2)\\cdots f(x_n),$$ donde $f(\\cdot)$ es la densidad (común) de cada $X_i$. Entonces, se define que $X_1, X_2, \\ldots, X_n$ es una *muestra aleatoria* de tamaño $n$ de una población con densidad $f(\\cdot)$.\n:::\n\n::: {#def-medmues style=\"text-align: justify\"}\n#### Media Muestral\n\nEl primer momento de la muestra es la *media muestral*, definida como\n\n$$ \\bar{X}= \\bar{X_n}=\\frac{1}{n}\\sum_{i=1}^n X_i, $$\n\ndonde $X_1, X_2, \\ldots, X_n$ es una muestra aleatoria de una densidad $f(\\cdot)$. $\\bar{X}$ es una función de las variables aleatorias $X_1, \\ldots, X_n$, y por lo tanto, en teoría se puede determinar la distribución de $\\bar{X}$.\n:::\n\n::: {#def-varmues style=\"text-align: justify\"}\n#### Varianza muestral\n\nSea $X_1, X_2, \\ldots, X_n$ una muestra aleatoria con densidad $f(\\cdot)$, entonces\n\n$$ S_n^2=S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2\\qquad \\text{para  } n>1 $$\n\nse define como la *varianza muestral*.\n:::\n\n::: {#thm-meanvar style=\"text-align: justify\"}\nSea $X_1, X_2, \\ldots, X_n$ una muestra aleatoria con densidad $f(\\cdot)$, la cual tiene media $\\mu$ y varianza finita $\\sigma^2$. Si $\\bar{X} = \\frac{1}{n}\\sum\\limits_{i=1}^n X_i$, entonces\n\n$$ \\mathrm{E}[\\bar X]=\\mu_{\\bar X}=\\mu\\qquad \\text{ y }\\qquad \\mathrm{Var}[\\bar X]=\\sigma_{\\bar X}^2 =\\frac{1}{n}\\sigma^2.$$\n\n::: proof\nVea @mood1986introduction.\n:::\n:::\n\n::: {#thm-TCL style=\"text-align: justify\"}\n### Teorema del límite central\n\nSupongamos que $f(\\cdot)$ es una densidad con media $\\mu$ y varianza finita $\\sigma^2$. Si $\\bar{X}_n$ es el promedio de una muestra aleatoria de tamaño $n$ extraída de $f(\\cdot)$ y definimos la variable aleatoria $Z_n$ como\n\n$$ Z_n = \\frac{\\bar{X_n}-\\mathrm E[\\bar X_n]}{\\sqrt{\\mathrm{Var}[\\bar X_n]}}=\\frac{\\bar X_n-\\mu}{\\sigma/\\sqrt n},$$ entonces, la distribución de $Z_n$ se acerca a la distribución normal estándar a medida que $n$ tiende a infinito.\n\n::: proof\nVea @wackerly2009estadística.\n:::\n:::\n\n## Datos multivariados\n\n::: {style=\"text-align: justify\"}\nCuando dos o más variables aleatorias son observadas en miembros de una muestra aleatoria, los datos resultantes se denominan datos multivariados. El caso especial de dos variables se refiere como datos bivariados.\n:::\n\n::: {.callout-caution collapse=\"true\" style=\"text-align: justify\" icon=\"false\"}\n### Ejemplo (Calificaciones finales)\n\nConsidere los datos en la @tbl-cal que representan una muestra de 30 estudiantes en una universidad grande que fueron asignados al azar a un curso de Introducción a la Informática. En la Tabla se muestran las puntuaciones del cuestionario, además de la calificación del examen final de cada estudiante.\n\n| Est. | Puntuación | Final | Est. | Puntuación | Final | Est. | Puntuación | Final  |\n|:-----|:-----------|:------|:-----|:-----------|:------|:-----|:-----------|:-------|\n| 1    | 7.4        | 79.8  | 11   | 7.6        | 80.7  | 21   | 8.0        | 84.2   |\n| 2    | 8.4        | 82.0  | 12   | 8.8        | 94.5  | 22   | 9.0        | 87.8   |\n| 3    | 8.8        | 76.1  | 13   | 6.1        | 50.1  | 23   | 8.9        | 94.1   |\n| 4    | 6.4        | 62.7  | 14   | 7.2        | 68.3  | 24   | 7.5        | 78.2   |\n| 5    | 10.0       | 98.2  | 15   | 6.6        | 64.4  | 25   | 5.5        | 62.4   |\n| 6    | 5.5        | 43.0  | 16   | 7.0        | 67.2  | 26   | 8.5        | 85.1   |\n| 7    | 7.3        | 76.5  | 17   | 5.3        | 53.9  | 27   | 7.4        | 77.8   |\n| 8    | 5.9        | 61.4  | 18   | 7.9        | 78.8  | 28   | 6.3        | 67.6   |\n| 9    | 7.1        | 78.5  | 19   | 8.1        | 85.7  | 29   | 7.7        | 70.2   |\n| 10   | 7.9        | 88.7  | 20   | 7.6        | 81.7  | 30   | 6.9        | 73.6   |\n|      |            |       |      |            |       | Suma | 222.6      | 2253.2 |\n\n: Puntuaciones y calificación final. {#tbl-cal}\n\nLos valores medios y las desviaciones estándar se proporcionan a continuación. Las puntuaciones del cuestionario están en el vector $x$ y las puntuaciones del examen final están en $y$. Se tienen los siguientes resultados:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nMedia(x) = 7.42\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesviación estándar(x) = 1.15\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMedia(y) = 75.11\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDesviación estándar(y) = 13.15\n```\n\n\n:::\n:::\n\n\n\n\n\nLos histogramas de estas dos variables se muestran en la @fig-histo. Allí se puede observar que los histogramas de ambas variables tienen una forma aproximadamente en campana, con las puntuaciones del examen final ligeramente sesgadas hacia la izquierda. Al examinar el histograma en la @fig-histo(b), se observa que la media de las puntuaciones del examen final parece ser de alrededor de 75, lo cual es coherente con los resultados anteriores. La mayoría de las puntuaciones están entre 60 y 90, con algunas por encima de 90 y algunas por debajo de 60.\n\n::: {#fig-histo}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](estadistica_files/figure-epub/unnamed-chunk-2-1.png)\n:::\n:::\n\n\n\n\n\nPuntajes y calificaciones finales para una muestra aleatoria de 30 estudiantes inscritos al curso.\n:::\n:::\n\n::: {style=\"text-align: justify\"}\nLos histogramas, las medias y las desviaciones estándar no proporcionan información sobre cómo dos variables están relacionadas entre sí. Como ejemplo, un instructor probablemente quisiera saber si los estudiantes que les fue bien en el cuestionario también tendieron a desempeñarse bien en el examen final, y viceversa. También podría querer saber si algunos estudiantes que les fue mal en el cuestionario mejoraron drásticamente su calificación en el examen final. Los histogramas y las estadísticas de muestra mostrados anteriormente no responden a estas preguntas.\n\nLo que se necesita son medidas de la relación entre las dos variables. Los parámetros poblacionales más comunes utilizados para medir tales relaciones son la covarianza, $\\gamma_{XY}$, y la correlación, $\\rho_{XY}$.\n:::\n\n::: {#def-cov style=\"text-align: justify\"}\n### Covarianza\n\nSi $X$ e $Y$ son dos variables aleatorias definidas en el mismo espacio de probabilidad, la *covarianza* de $X$ e $Y$, denotada como $\\mathrm{Cov}[X,Y]$ o $\\gamma_{XY}$, se define como $$\\gamma_{XY}=\\mathrm{Cov}[X,Y]=\\mathrm E[(X-\\mu_X)(Y-\\mu_Y)]$$ siempre que la esperanza indicada exista.\n:::\n\n::: {#def-corr style=\"text-align: justify\"}\n### Correlación\n\nLa *correlación*, denotada como $\\rho[X,Y]$ o $\\rho_{XY}$, de las variables aleatorias $X$ e $Y$, se define como\n\n$$ \\rho_{XY}=\\frac{\\gamma_{XY}}{\\sigma_X \\sigma_Y} $$\n\nsiempre que $\\gamma_{XY}, \\sigma_X$ y $\\sigma_Y$ existan, con $\\sigma_X, \\sigma_Y>0$.\n:::\n\n::: {style=\"text-align: justify\"}\nTécnicamente, la covarianza es el valor esperado (o promedio teórico) del producto cruzado $(X-\\mu_X)(Y-\\mu_Y)$. Es una medida de cómo dos variables \"se mueven juntas\". Para facilitar la interpretación, generalmente se usa la correlación, que es una versión \"estandarizada\" de la covarianza que tiene la propiedad $-1 \\leq \\rho_{XY}\\leq 1$ para cualquier par de variables aleatorias $X$ e $Y$.\n:::\n\n::: {.remark style=\"text-align: justify\"}\na.  $\\gamma_{XX}=\\mathrm E[(X-\\mu_X)(X-\\mu_X)]=\\mathrm E[(X-\\mu_X)^2]=\\sigma_X^2$.\n\nb.  $\\rho_{XX}=\\frac{\\sigma_X^2}{\\sigma_X \\sigma_X}=1$.\n:::\n\n::: {#cor-covind}\nSi $X$ e $Y$ son independientes, entonces $\\gamma_{XY}=0$.\n\n::: {.proof style=\"text-align: justify\"}\nObserve que\n\n$$ \\begin{split}\\mathrm{Cov}[X,Y]=\\mathrm E[(X-\\mu_X)(Y-\\mu_Y)]=\\mathrm E[X-\\mu_X]\\mathrm E[Y-\\mu_Y]=0,\\end{split} $$\n\nya que $\\mathrm E[(X-\\mu_X)]=0$.\n:::\n:::\n\n::: {#thm-propiedades style=\"text-align: justify\"}\nSean $X$ e $Y$ variables aleatorias definidas sobre el mismo espacio de probabilidad tales que $E(X^2) < \\infty$ y $E(Y^2) < \\infty$. Entonces:\n\ni.  $\\mathrm{Cov}[X, Y]= \\mathrm{E}[XY]-\\mathrm{E}[X]\\mathrm{E}[Y]$.\n\nii. $\\mathrm{Cov}[X, Y]= \\mathrm{Cov}[Y,X]$.\n\niii. $\\mathrm{Var}[X]= \\mathrm{Cov}[X,X]$.\n\niv. $\\mathrm{Cov}[aX+b, Y]= a\\mathrm{Cov}[X, Y]$ para cualquier $a,b \\in\\mathbb{R}$.\n\n::: proof\nVea @castañeda2014introduction.\n:::\n:::\n\n::: {#def-uncorr style=\"text-align: justify\"}\n### Variables aleatorias no correlacionadas\n\nLas variables aleatorias $X$ e $Y$ se definen como no correlacionadas si y solo si $\\mathrm{Cov}[X,Y]=0$.\n:::\n\n::: {.remark style=\"text-align: justify\"}\nLa afirmación contraria al corolario anterior no siempre es cierta; es decir, $\\mathrm{Cov}[X,Y]=0$ no siempre implica que $X$ e $Y$ sean independientes.\n:::\n",
    "supporting": [
      "estadistica_files\\figure-epub"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}